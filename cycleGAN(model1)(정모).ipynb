{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKzJ0QD23lC_",
        "outputId": "f3e16e04-6673-4dc0-94d3-68d09523e8d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "#!pip install torch torchvision pillow numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab 마운트"
      ],
      "metadata": {
        "id": "cJSlUqHVjcQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FOLDERNAME = 'test'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# Change directory to current folder\n",
        "%cd /content/drive/MyDrive/$FOLDERNAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuJp_EpC3mck",
        "outputId": "0e4ad3f9-98a8-45d5-88d7-3b232453e0d8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필요한 라이브러리 import"
      ],
      "metadata": {
        "id": "V3nwJu5_je57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import itertools\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "wxUTF-cX3oCq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 만약 파일을 실행시켜보고 싶다면 아래 코드를 실행시켜야 함\n",
        "- directory에 유의 -> dataset4에 대한 경로를 집어넣어야 함\n",
        "- 아래 코드는 각 파일 안에 사진 명을 older_face1, older_face2, ..., younger_face1, younger_face2,... 이런 형식으로 변환해주는 코드"
      ],
      "metadata": {
        "id": "ufXz5mr8lFCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory containing the images\n",
        "directory = '/content/drive/MyDrive/deep_learning/dataset4/older'\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(directory)\n",
        "\n",
        "# Filter for image files (jpg and png)\n",
        "image_files = [f for f in files if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# Sort the files to maintain order\n",
        "image_files.sort()\n",
        "\n",
        "# Loop through and rename each file\n",
        "for idx, filename in enumerate(image_files):\n",
        "    # Construct the new file name\n",
        "    new_name = f'older_face{idx+1}{os.path.splitext(filename)[1]}'  # Preserves the original file extension\n",
        "\n",
        "    # Full paths for the source and destination files\n",
        "    src = os.path.join(directory, filename)\n",
        "    dst = os.path.join(directory, new_name)\n",
        "\n",
        "    # Rename the file\n",
        "    os.rename(src, dst)\n",
        "\n",
        "    print(f'Renamed {src} to {dst}')"
      ],
      "metadata": {
        "id": "4BLneJF-lIU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory containing the images\n",
        "directory = '/content/drive/MyDrive/deep_learning/dataset4/younger'\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(directory)\n",
        "\n",
        "# Filter for image files (jpg and png)\n",
        "image_files = [f for f in files if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# Sort the files to maintain order\n",
        "image_files.sort()\n",
        "\n",
        "# Loop through and rename each file\n",
        "for idx, filename in enumerate(image_files):\n",
        "    # Construct the new file name\n",
        "    new_name = f'younger_face{idx+1}{os.path.splitext(filename)[1]}'  # Preserves the original file extension\n",
        "\n",
        "    # Full paths for the source and destination files\n",
        "    src = os.path.join(directory, filename)\n",
        "    dst = os.path.join(directory, new_name)\n",
        "\n",
        "    # Rename the file\n",
        "    os.rename(src, dst)\n",
        "\n",
        "    print(f'Renamed {src} to {dst}')"
      ],
      "metadata": {
        "id": "xz99_DiMldi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사진 파일 불러오기"
      ],
      "metadata": {
        "id": "FOvwjNXfjiZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'dataset' 파일 안에 'older' 파일과 'younger' 파일이 있는 형태여야 함"
      ],
      "metadata": {
        "id": "O0fQb7eojlFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base directories\n",
        "target_dir = '/content/drive/MyDrive/deep_learning/dataset4'\n",
        "older_dir = os.path.join(target_dir, 'older')\n",
        "younger_dir = os.path.join(target_dir, 'younger')"
      ],
      "metadata": {
        "id": "vqnm0IHJ3rpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformations, dataloaders, batch size"
      ],
      "metadata": {
        "id": "8j5woZwejsRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Run the PyTorch code\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    #transforms.RandomRotation(degrees=10),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Define dataset and dataloaders\n",
        "older_dataset = datasets.ImageFolder(root=older_dir, transform=transform)\n",
        "younger_dataset = datasets.ImageFolder(root=younger_dir, transform=transform)\n",
        "\n",
        "# Ensure drop_last=True to handle incomplete batches\n",
        "batch_size = 32\n",
        "older_dataloader = DataLoader(older_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "younger_dataloader = DataLoader(younger_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "# Example: Print first few images to verify\n",
        "older_img, older_label = next(iter(older_dataloader))\n",
        "younger_img, younger_label = next(iter(younger_dataloader))\n",
        "\n",
        "print(\"Shape of older image batch:\", older_img.shape)\n",
        "print(\"Shape of younger image batch:\", younger_img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0-4QGw54KyH",
        "outputId": "a85f83ce-8596-4c6b-e195-baf0e6d9e5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of older image batch: torch.Size([32, 3, 256, 256])\n",
            "Shape of younger image batch: torch.Size([32, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generator, discriminator"
      ],
      "metadata": {
        "id": "7cmT7ySMjyLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Define the Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n"
      ],
      "metadata": {
        "id": "WlUwu-yG4VBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cycleGAN 정의하기"
      ],
      "metadata": {
        "id": "CMKDEBN1j1Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CycleGAN, self).__init__()\n",
        "        self.G_XtoY = Generator()\n",
        "        self.G_YtoX = Generator()\n",
        "        self.D_X = Discriminator()\n",
        "        self.D_Y = Discriminator()\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cycle_gan = CycleGAN().to(device)\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mi-07bt4WiQ",
        "outputId": "633d5d85-39b6-41c3-9b20-473e09bbae88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loss function, optimizer"
      ],
      "metadata": {
        "id": "DMepj-kij4y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import torch.optim as optim\n",
        "\n",
        "# Loss functions\n",
        "criterion_GAN = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(itertools.chain(cycle_gan.G_XtoY.parameters(), cycle_gan.G_YtoX.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_X = optim.Adam(cycle_gan.D_X.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_Y = optim.Adam(cycle_gan.D_Y.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
      ],
      "metadata": {
        "id": "BgGGp39v4Yc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model training"
      ],
      "metadata": {
        "id": "ET0FiQM5j7qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data_X, data_Y) in enumerate(zip(older_dataloader, younger_dataloader)):\n",
        "        # Get real images\n",
        "        real_X = data_X[0].to(device)\n",
        "        real_Y = data_Y[0].to(device)\n",
        "\n",
        "        # Adversarial ground truths with the same shape as discriminator outputs\n",
        "        valid = torch.ones(real_X.size(0), 1, 15, 15, dtype=torch.float, requires_grad=False).to(device)\n",
        "        fake = torch.zeros(real_X.size(0), 1, 15, 15, dtype=torch.float, requires_grad=False).to(device)\n",
        "\n",
        "        # ----------------------\n",
        "        #  Train Generators\n",
        "        # ----------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        loss_id_X = criterion_identity(cycle_gan.G_YtoX(real_X), real_X)\n",
        "        loss_id_Y = criterion_identity(cycle_gan.G_XtoY(real_Y), real_Y)\n",
        "\n",
        "        loss_identity = (loss_id_X + loss_id_Y) / 2\n",
        "\n",
        "        # GAN loss\n",
        "        fake_Y = cycle_gan.G_XtoY(real_X)\n",
        "        loss_GAN_XtoY = criterion_GAN(cycle_gan.D_Y(fake_Y), valid)\n",
        "\n",
        "        fake_X = cycle_gan.G_YtoX(real_Y)\n",
        "        loss_GAN_YtoX = criterion_GAN(cycle_gan.D_X(fake_X), valid)\n",
        "\n",
        "        loss_GAN = (loss_GAN_XtoY + loss_GAN_YtoX) / 2\n",
        "\n",
        "        # Cycle loss\n",
        "        recov_X = cycle_gan.G_YtoX(fake_Y)\n",
        "        loss_cycle_X = criterion_cycle(recov_X, real_X)\n",
        "\n",
        "        recov_Y = cycle_gan.G_XtoY(fake_X)\n",
        "        loss_cycle_Y = criterion_cycle(recov_Y, real_Y)\n",
        "\n",
        "        loss_cycle = (loss_cycle_X + loss_cycle_Y) / 2\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_GAN + 10.0 * loss_cycle + 5.0 * loss_identity\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Adjust output range of generated images\n",
        "        fake_Y = (fake_Y + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
        "        fake_X = (fake_X + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
        "\n",
        "        # ----------------------\n",
        "        #  Train Discriminators\n",
        "        # ----------------------\n",
        "\n",
        "        # Discriminator X\n",
        "        optimizer_D_X.zero_grad()\n",
        "\n",
        "        loss_real_X = criterion_GAN(cycle_gan.D_X(real_X), valid)\n",
        "        loss_fake_X = criterion_GAN(cycle_gan.D_X(fake_X.detach()), fake)\n",
        "\n",
        "        loss_D_X = (loss_real_X + loss_fake_X) / 2\n",
        "\n",
        "        loss_D_X.backward()\n",
        "        optimizer_D_X.step()\n",
        "\n",
        "        # Discriminator Y\n",
        "        optimizer_D_Y.zero_grad()\n",
        "\n",
        "        loss_real_Y = criterion_GAN(cycle_gan.D_Y(real_Y), valid)\n",
        "        loss_fake_Y = criterion_GAN(cycle_gan.D_Y(fake_Y.detach()), fake)\n",
        "\n",
        "        loss_D_Y = (loss_real_Y + loss_fake_Y) / 2\n",
        "\n",
        "        loss_D_Y.backward()\n",
        "        optimizer_D_Y.step()\n",
        "\n",
        "        print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(older_dataloader)}] \"\n",
        "              f\"[D loss: {loss_D_X.item() + loss_D_Y.item()}] [G loss: {loss_G.item()}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NXprUH6f4aaV",
        "outputId": "b158485f-e79b-437f-fb15-c7f24973768c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/30] [Batch 0/66] [D loss: 0.5218207538127899] [G loss: 12.925529479980469]\n",
            "[Epoch 0/30] [Batch 1/66] [D loss: 0.41605815291404724] [G loss: 11.643762588500977]\n",
            "[Epoch 0/30] [Batch 2/66] [D loss: 0.4779323637485504] [G loss: 10.196147918701172]\n",
            "[Epoch 0/30] [Batch 3/66] [D loss: 0.3108989894390106] [G loss: 8.92475700378418]\n",
            "[Epoch 0/30] [Batch 4/66] [D loss: 0.26772312819957733] [G loss: 8.091504096984863]\n",
            "[Epoch 0/30] [Batch 5/66] [D loss: 0.2384706437587738] [G loss: 7.534849643707275]\n",
            "[Epoch 0/30] [Batch 6/66] [D loss: 0.1711549311876297] [G loss: 6.985177993774414]\n",
            "[Epoch 0/30] [Batch 7/66] [D loss: 0.10584665834903717] [G loss: 6.555181503295898]\n",
            "[Epoch 0/30] [Batch 8/66] [D loss: 0.07794133573770523] [G loss: 6.243775844573975]\n",
            "[Epoch 0/30] [Batch 9/66] [D loss: 0.05199182219803333] [G loss: 5.860116004943848]\n",
            "[Epoch 0/30] [Batch 10/66] [D loss: 0.03900352492928505] [G loss: 5.57840633392334]\n",
            "[Epoch 0/30] [Batch 11/66] [D loss: 0.028393983840942383] [G loss: 5.440671920776367]\n",
            "[Epoch 0/30] [Batch 12/66] [D loss: 0.02474069967865944] [G loss: 5.244015693664551]\n",
            "[Epoch 0/30] [Batch 13/66] [D loss: 0.01899522915482521] [G loss: 4.93535041809082]\n",
            "[Epoch 0/30] [Batch 14/66] [D loss: 0.016260335221886635] [G loss: 4.786268711090088]\n",
            "[Epoch 0/30] [Batch 15/66] [D loss: 0.013663310557603836] [G loss: 4.554642200469971]\n",
            "[Epoch 0/30] [Batch 16/66] [D loss: 0.011910464614629745] [G loss: 4.350792407989502]\n",
            "[Epoch 0/30] [Batch 17/66] [D loss: 0.010768258478492498] [G loss: 4.294882297515869]\n",
            "[Epoch 0/30] [Batch 18/66] [D loss: 0.010153236798942089] [G loss: 4.124049186706543]\n",
            "[Epoch 0/30] [Batch 19/66] [D loss: 0.008815659210085869] [G loss: 3.950744152069092]\n",
            "[Epoch 0/30] [Batch 20/66] [D loss: 0.008263417519629002] [G loss: 3.8811049461364746]\n",
            "[Epoch 0/30] [Batch 21/66] [D loss: 0.007783830165863037] [G loss: 3.8558335304260254]\n",
            "[Epoch 0/30] [Batch 22/66] [D loss: 0.007521711988374591] [G loss: 3.815652370452881]\n",
            "[Epoch 0/30] [Batch 23/66] [D loss: 0.00626287842169404] [G loss: 3.6539411544799805]\n",
            "[Epoch 0/30] [Batch 24/66] [D loss: 0.005911548156291246] [G loss: 3.563453435897827]\n",
            "[Epoch 0/30] [Batch 25/66] [D loss: 0.005771387834101915] [G loss: 3.4750430583953857]\n",
            "[Epoch 0/30] [Batch 26/66] [D loss: 0.005251547321677208] [G loss: 3.3800864219665527]\n",
            "[Epoch 0/30] [Batch 27/66] [D loss: 0.004954863106831908] [G loss: 3.309520959854126]\n",
            "[Epoch 0/30] [Batch 28/66] [D loss: 0.004628908820450306] [G loss: 3.240513324737549]\n",
            "[Epoch 0/30] [Batch 29/66] [D loss: 0.004644956439733505] [G loss: 3.2215652465820312]\n",
            "[Epoch 0/30] [Batch 30/66] [D loss: 0.004085337743163109] [G loss: 3.166271448135376]\n",
            "[Epoch 0/30] [Batch 31/66] [D loss: 0.003955835476517677] [G loss: 3.053651809692383]\n",
            "[Epoch 0/30] [Batch 32/66] [D loss: 0.003895863890647888] [G loss: 2.960050106048584]\n",
            "[Epoch 0/30] [Batch 33/66] [D loss: 0.003531410009600222] [G loss: 2.8673949241638184]\n",
            "[Epoch 0/30] [Batch 34/66] [D loss: 0.003329829196445644] [G loss: 2.8620176315307617]\n",
            "[Epoch 0/30] [Batch 35/66] [D loss: 0.0034172453451901674] [G loss: 2.9984259605407715]\n",
            "[Epoch 0/30] [Batch 36/66] [D loss: 0.0032721771858632565] [G loss: 2.8232054710388184]\n",
            "[Epoch 0/30] [Batch 37/66] [D loss: 0.0030415136134251952] [G loss: 2.7677104473114014]\n",
            "[Epoch 0/30] [Batch 38/66] [D loss: 0.0028161111986264586] [G loss: 2.650386333465576]\n",
            "[Epoch 0/30] [Batch 39/66] [D loss: 0.0027296622283756733] [G loss: 2.6220970153808594]\n",
            "[Epoch 0/30] [Batch 40/66] [D loss: 0.002825366333127022] [G loss: 2.597465991973877]\n",
            "[Epoch 0/30] [Batch 41/66] [D loss: 0.0024778805673122406] [G loss: 2.6489803791046143]\n",
            "[Epoch 0/30] [Batch 42/66] [D loss: 0.0024663459043949842] [G loss: 2.556711196899414]\n",
            "[Epoch 0/30] [Batch 43/66] [D loss: 0.0023664210457354784] [G loss: 2.4370172023773193]\n",
            "[Epoch 0/30] [Batch 44/66] [D loss: 0.0023326060036197305] [G loss: 2.4509050846099854]\n",
            "[Epoch 0/30] [Batch 45/66] [D loss: 0.0021453325171023607] [G loss: 2.393106460571289]\n",
            "[Epoch 0/30] [Batch 46/66] [D loss: 0.002076878445222974] [G loss: 2.349135637283325]\n",
            "[Epoch 0/30] [Batch 47/66] [D loss: 0.002114138100296259] [G loss: 2.3453943729400635]\n",
            "[Epoch 0/30] [Batch 48/66] [D loss: 0.0020053127082064748] [G loss: 2.346785068511963]\n",
            "[Epoch 0/30] [Batch 49/66] [D loss: 0.002067607070785016] [G loss: 2.2891385555267334]\n",
            "[Epoch 0/30] [Batch 50/66] [D loss: 0.00190751813352108] [G loss: 2.3418431282043457]\n",
            "[Epoch 0/30] [Batch 51/66] [D loss: 0.0017980131669901311] [G loss: 2.3184590339660645]\n",
            "[Epoch 0/30] [Batch 52/66] [D loss: 0.0017421898082830012] [G loss: 2.2555994987487793]\n",
            "[Epoch 0/30] [Batch 53/66] [D loss: 0.0017854529432952404] [G loss: 2.2941250801086426]\n",
            "[Epoch 0/30] [Batch 54/66] [D loss: 0.0016951014986261725] [G loss: 2.1901891231536865]\n",
            "[Epoch 0/30] [Batch 55/66] [D loss: 0.001597977476194501] [G loss: 2.098877429962158]\n",
            "[Epoch 0/30] [Batch 56/66] [D loss: 0.0015692696906626225] [G loss: 2.0927019119262695]\n",
            "[Epoch 0/30] [Batch 57/66] [D loss: 0.0015451755607500672] [G loss: 2.1160812377929688]\n",
            "[Epoch 0/30] [Batch 58/66] [D loss: 0.0015321397804655135] [G loss: 2.039307117462158]\n",
            "[Epoch 0/30] [Batch 59/66] [D loss: 0.0014103532303124666] [G loss: 1.9816503524780273]\n",
            "[Epoch 0/30] [Batch 60/66] [D loss: 0.0014591816579923034] [G loss: 2.0306382179260254]\n",
            "[Epoch 0/30] [Batch 61/66] [D loss: 0.0014598225243389606] [G loss: 1.9626246690750122]\n",
            "[Epoch 0/30] [Batch 62/66] [D loss: 0.0013034315779805183] [G loss: 1.9190629720687866]\n",
            "[Epoch 0/30] [Batch 63/66] [D loss: 0.0012942381436005235] [G loss: 1.9168314933776855]\n",
            "[Epoch 0/30] [Batch 64/66] [D loss: 0.0013573509058915079] [G loss: 1.9828855991363525]\n",
            "[Epoch 1/30] [Batch 0/66] [D loss: 0.0013100553187541664] [G loss: 1.9301345348358154]\n",
            "[Epoch 1/30] [Batch 1/66] [D loss: 0.0012579396134242415] [G loss: 1.8750168085098267]\n",
            "[Epoch 1/30] [Batch 2/66] [D loss: 0.0012514895061030984] [G loss: 1.853773832321167]\n",
            "[Epoch 1/30] [Batch 3/66] [D loss: 0.001219606027007103] [G loss: 1.842553734779358]\n",
            "[Epoch 1/30] [Batch 4/66] [D loss: 0.0011735910084098577] [G loss: 1.8949179649353027]\n",
            "[Epoch 1/30] [Batch 5/66] [D loss: 0.0011350891436450183] [G loss: 1.9296283721923828]\n",
            "[Epoch 1/30] [Batch 6/66] [D loss: 0.0011195886181667447] [G loss: 1.7639418840408325]\n",
            "[Epoch 1/30] [Batch 7/66] [D loss: 0.00108170643215999] [G loss: 1.8466989994049072]\n",
            "[Epoch 1/30] [Batch 8/66] [D loss: 0.001091902842745185] [G loss: 1.7654261589050293]\n",
            "[Epoch 1/30] [Batch 9/66] [D loss: 0.0009797863312996924] [G loss: 1.8009698390960693]\n",
            "[Epoch 1/30] [Batch 10/66] [D loss: 0.0010010048281401396] [G loss: 1.7642244100570679]\n",
            "[Epoch 1/30] [Batch 11/66] [D loss: 0.000991935026831925] [G loss: 1.7382310628890991]\n",
            "[Epoch 1/30] [Batch 12/66] [D loss: 0.00096607900923118] [G loss: 1.8053622245788574]\n",
            "[Epoch 1/30] [Batch 13/66] [D loss: 0.0009738882654346526] [G loss: 1.8793631792068481]\n",
            "[Epoch 1/30] [Batch 14/66] [D loss: 0.0009189796983264387] [G loss: 1.698418378829956]\n",
            "[Epoch 1/30] [Batch 15/66] [D loss: 0.0009445153118576854] [G loss: 1.6728441715240479]\n",
            "[Epoch 1/30] [Batch 16/66] [D loss: 0.0009228151757270098] [G loss: 1.7689958810806274]\n",
            "[Epoch 1/30] [Batch 17/66] [D loss: 0.0009153050486929715] [G loss: 1.7598422765731812]\n",
            "[Epoch 1/30] [Batch 18/66] [D loss: 0.0009082227770704776] [G loss: 1.658043384552002]\n",
            "[Epoch 1/30] [Batch 19/66] [D loss: 0.0008795461617410183] [G loss: 1.590105414390564]\n",
            "[Epoch 1/30] [Batch 20/66] [D loss: 0.0008340331260114908] [G loss: 1.5538041591644287]\n",
            "[Epoch 1/30] [Batch 21/66] [D loss: 0.0008232745749410242] [G loss: 1.5732762813568115]\n",
            "[Epoch 1/30] [Batch 22/66] [D loss: 0.0008440823585260659] [G loss: 1.590261697769165]\n",
            "[Epoch 1/30] [Batch 23/66] [D loss: 0.0007749101496301591] [G loss: 1.6326878070831299]\n",
            "[Epoch 1/30] [Batch 24/66] [D loss: 0.000777824257966131] [G loss: 1.5433809757232666]\n",
            "[Epoch 1/30] [Batch 25/66] [D loss: 0.0007389180536847562] [G loss: 1.5592445135116577]\n",
            "[Epoch 1/30] [Batch 26/66] [D loss: 0.0007378938607871532] [G loss: 1.5183346271514893]\n",
            "[Epoch 1/30] [Batch 27/66] [D loss: 0.0007445312803611159] [G loss: 1.5309882164001465]\n",
            "[Epoch 1/30] [Batch 28/66] [D loss: 0.0007527192065026611] [G loss: 1.51778244972229]\n",
            "[Epoch 1/30] [Batch 29/66] [D loss: 0.0007240659033413976] [G loss: 1.4809150695800781]\n",
            "[Epoch 1/30] [Batch 30/66] [D loss: 0.0007024305523373187] [G loss: 1.4701310396194458]\n",
            "[Epoch 1/30] [Batch 31/66] [D loss: 0.0007243289146572351] [G loss: 1.4813637733459473]\n",
            "[Epoch 1/30] [Batch 32/66] [D loss: 0.0007335207192227244] [G loss: 1.5073435306549072]\n",
            "[Epoch 1/30] [Batch 33/66] [D loss: 0.000686076411511749] [G loss: 1.7957524061203003]\n",
            "[Epoch 1/30] [Batch 34/66] [D loss: 0.000662555277813226] [G loss: 1.817430019378662]\n",
            "[Epoch 1/30] [Batch 35/66] [D loss: 0.0007143780239857733] [G loss: 1.846724510192871]\n",
            "[Epoch 1/30] [Batch 36/66] [D loss: 0.0006660220678895712] [G loss: 2.0342442989349365]\n",
            "[Epoch 1/30] [Batch 37/66] [D loss: 0.0007365619821939617] [G loss: 2.0306077003479004]\n",
            "[Epoch 1/30] [Batch 38/66] [D loss: 0.0007702196598984301] [G loss: 1.7904162406921387]\n",
            "[Epoch 1/30] [Batch 39/66] [D loss: 0.0007340603042393923] [G loss: 1.72163987159729]\n",
            "[Epoch 1/30] [Batch 40/66] [D loss: 0.0007922034710645676] [G loss: 1.7168188095092773]\n",
            "[Epoch 1/30] [Batch 41/66] [D loss: 0.0007159017259255052] [G loss: 1.5667414665222168]\n",
            "[Epoch 1/30] [Batch 42/66] [D loss: 0.0007984656549524516] [G loss: 1.4385823011398315]\n",
            "[Epoch 1/30] [Batch 43/66] [D loss: 0.0008404639665968716] [G loss: 1.3632102012634277]\n",
            "[Epoch 1/30] [Batch 44/66] [D loss: 0.0008778104675002396] [G loss: 1.3031035661697388]\n",
            "[Epoch 1/30] [Batch 45/66] [D loss: 0.0009462543530389667] [G loss: 1.3318395614624023]\n",
            "[Epoch 1/30] [Batch 46/66] [D loss: 0.0006979367462918162] [G loss: 1.2980252504348755]\n",
            "[Epoch 1/30] [Batch 47/66] [D loss: 0.0006956435681786388] [G loss: 1.2958731651306152]\n",
            "[Epoch 1/30] [Batch 48/66] [D loss: 0.0006386004388332367] [G loss: 1.3182826042175293]\n",
            "[Epoch 1/30] [Batch 49/66] [D loss: 0.000633106246823445] [G loss: 1.3866528272628784]\n",
            "[Epoch 1/30] [Batch 50/66] [D loss: 0.0005685145151801407] [G loss: 1.3195462226867676]\n",
            "[Epoch 1/30] [Batch 51/66] [D loss: 0.0006323865090962499] [G loss: 1.3054094314575195]\n",
            "[Epoch 1/30] [Batch 52/66] [D loss: 0.0005346431280486286] [G loss: 1.2896904945373535]\n",
            "[Epoch 1/30] [Batch 53/66] [D loss: 0.0005567999905906618] [G loss: 1.2601054906845093]\n",
            "[Epoch 1/30] [Batch 54/66] [D loss: 0.0005563843296840787] [G loss: 1.2409183979034424]\n",
            "[Epoch 1/30] [Batch 55/66] [D loss: 0.0005221163737587631] [G loss: 1.281160593032837]\n",
            "[Epoch 1/30] [Batch 56/66] [D loss: 0.0005300287739373744] [G loss: 1.214118242263794]\n",
            "[Epoch 1/30] [Batch 57/66] [D loss: 0.0005089979968033731] [G loss: 1.1678030490875244]\n",
            "[Epoch 1/30] [Batch 58/66] [D loss: 0.0005104407318867743] [G loss: 1.1788887977600098]\n",
            "[Epoch 1/30] [Batch 59/66] [D loss: 0.0005145713657839224] [G loss: 1.1602421998977661]\n",
            "[Epoch 1/30] [Batch 60/66] [D loss: 0.0005014643538743258] [G loss: 1.2794771194458008]\n",
            "[Epoch 1/30] [Batch 61/66] [D loss: 0.0004870593547821045] [G loss: 1.3310705423355103]\n",
            "[Epoch 1/30] [Batch 62/66] [D loss: 0.0005074936198070645] [G loss: 1.2423014640808105]\n",
            "[Epoch 1/30] [Batch 63/66] [D loss: 0.0004458666662685573] [G loss: 1.1439306735992432]\n",
            "[Epoch 1/30] [Batch 64/66] [D loss: 0.00045982669689692557] [G loss: 1.1419227123260498]\n",
            "[Epoch 2/30] [Batch 0/66] [D loss: 0.00046390664647333324] [G loss: 1.1236827373504639]\n",
            "[Epoch 2/30] [Batch 1/66] [D loss: 0.000440778152551502] [G loss: 1.0685398578643799]\n",
            "[Epoch 2/30] [Batch 2/66] [D loss: 0.00043788844777736813] [G loss: 1.068151831626892]\n",
            "[Epoch 2/30] [Batch 3/66] [D loss: 0.00046027802454773337] [G loss: 1.1511280536651611]\n",
            "[Epoch 2/30] [Batch 4/66] [D loss: 0.0004452855500858277] [G loss: 1.1637279987335205]\n",
            "[Epoch 2/30] [Batch 5/66] [D loss: 0.0004157752846367657] [G loss: 1.1121081113815308]\n",
            "[Epoch 2/30] [Batch 6/66] [D loss: 0.0004233521467540413] [G loss: 1.1572626829147339]\n",
            "[Epoch 2/30] [Batch 7/66] [D loss: 0.0004430288536241278] [G loss: 1.1198132038116455]\n",
            "[Epoch 2/30] [Batch 8/66] [D loss: 0.0004270731151336804] [G loss: 1.1563278436660767]\n",
            "[Epoch 2/30] [Batch 9/66] [D loss: 0.0004095300246262923] [G loss: 1.17240309715271]\n",
            "[Epoch 2/30] [Batch 10/66] [D loss: 0.00040555473242420703] [G loss: 1.0896258354187012]\n",
            "[Epoch 2/30] [Batch 11/66] [D loss: 0.00040488745435141027] [G loss: 1.1148293018341064]\n",
            "[Epoch 2/30] [Batch 12/66] [D loss: 0.00039896539237815887] [G loss: 1.101360559463501]\n",
            "[Epoch 2/30] [Batch 13/66] [D loss: 0.0004327653005020693] [G loss: 1.0295124053955078]\n",
            "[Epoch 2/30] [Batch 14/66] [D loss: 0.0004782395844813436] [G loss: 1.0270005464553833]\n",
            "[Epoch 2/30] [Batch 15/66] [D loss: 0.00044162129051983356] [G loss: 0.9758972525596619]\n",
            "[Epoch 2/30] [Batch 16/66] [D loss: 0.0004532983875833452] [G loss: 1.0745861530303955]\n",
            "[Epoch 2/30] [Batch 17/66] [D loss: 0.0003991117118857801] [G loss: 1.0709280967712402]\n",
            "[Epoch 2/30] [Batch 18/66] [D loss: 0.00044282394810579717] [G loss: 1.0259082317352295]\n",
            "[Epoch 2/30] [Batch 19/66] [D loss: 0.0003983215137850493] [G loss: 1.021828055381775]\n",
            "[Epoch 2/30] [Batch 20/66] [D loss: 0.0003808222827501595] [G loss: 1.0744071006774902]\n",
            "[Epoch 2/30] [Batch 21/66] [D loss: 0.0003987238451372832] [G loss: 1.0810309648513794]\n",
            "[Epoch 2/30] [Batch 22/66] [D loss: 0.000365939165931195] [G loss: 0.93560791015625]\n",
            "[Epoch 2/30] [Batch 23/66] [D loss: 0.0003604169323807582] [G loss: 0.9995818734169006]\n",
            "[Epoch 2/30] [Batch 24/66] [D loss: 0.0003675250627566129] [G loss: 0.93451988697052]\n",
            "[Epoch 2/30] [Batch 25/66] [D loss: 0.0003593845758587122] [G loss: 0.9810486435890198]\n",
            "[Epoch 2/30] [Batch 26/66] [D loss: 0.0003810869820881635] [G loss: 1.0040274858474731]\n",
            "[Epoch 2/30] [Batch 27/66] [D loss: 0.00033362033718731254] [G loss: 1.0305333137512207]\n",
            "[Epoch 2/30] [Batch 28/66] [D loss: 0.0003468626964604482] [G loss: 1.0577456951141357]\n",
            "[Epoch 2/30] [Batch 29/66] [D loss: 0.00034092603891622275] [G loss: 0.9454762935638428]\n",
            "[Epoch 2/30] [Batch 30/66] [D loss: 0.0003362984716659412] [G loss: 0.9477831125259399]\n",
            "[Epoch 2/30] [Batch 31/66] [D loss: 0.0003473109391052276] [G loss: 0.9585580825805664]\n",
            "[Epoch 2/30] [Batch 32/66] [D loss: 0.00034361533471383154] [G loss: 0.9648877382278442]\n",
            "[Epoch 2/30] [Batch 33/66] [D loss: 0.0003237231576349586] [G loss: 1.0195926427841187]\n",
            "[Epoch 2/30] [Batch 34/66] [D loss: 0.0003196942125214264] [G loss: 0.9475033283233643]\n",
            "[Epoch 2/30] [Batch 35/66] [D loss: 0.00034105700615327805] [G loss: 0.8960328102111816]\n",
            "[Epoch 2/30] [Batch 36/66] [D loss: 0.00041407858952879906] [G loss: 0.9525993466377258]\n",
            "[Epoch 2/30] [Batch 37/66] [D loss: 0.0003645028918981552] [G loss: 0.878113865852356]\n",
            "[Epoch 2/30] [Batch 38/66] [D loss: 0.00035387824755162] [G loss: 0.9441425800323486]\n",
            "[Epoch 2/30] [Batch 39/66] [D loss: 0.00035248299536760896] [G loss: 1.004774570465088]\n",
            "[Epoch 2/30] [Batch 40/66] [D loss: 0.0003375710512045771] [G loss: 0.849398672580719]\n",
            "[Epoch 2/30] [Batch 41/66] [D loss: 0.000331621355144307] [G loss: 0.9446021318435669]\n",
            "[Epoch 2/30] [Batch 42/66] [D loss: 0.00030346211860887706] [G loss: 0.9937502145767212]\n",
            "[Epoch 2/30] [Batch 43/66] [D loss: 0.00030819285893812776] [G loss: 0.8827040195465088]\n",
            "[Epoch 2/30] [Batch 44/66] [D loss: 0.00031245971331372857] [G loss: 0.871245801448822]\n",
            "[Epoch 2/30] [Batch 45/66] [D loss: 0.000289819116005674] [G loss: 0.8582527041435242]\n",
            "[Epoch 2/30] [Batch 46/66] [D loss: 0.00029886947595514357] [G loss: 0.8468163013458252]\n",
            "[Epoch 2/30] [Batch 47/66] [D loss: 0.00029343771166168153] [G loss: 0.8350101709365845]\n",
            "[Epoch 2/30] [Batch 48/66] [D loss: 0.00027491994842421263] [G loss: 0.8760619163513184]\n",
            "[Epoch 2/30] [Batch 49/66] [D loss: 0.0003016454429598525] [G loss: 0.8679237365722656]\n",
            "[Epoch 2/30] [Batch 50/66] [D loss: 0.0002909204922616482] [G loss: 0.895627498626709]\n",
            "[Epoch 2/30] [Batch 51/66] [D loss: 0.0002828570723067969] [G loss: 1.09283447265625]\n",
            "[Epoch 2/30] [Batch 52/66] [D loss: 0.0002904698339989409] [G loss: 0.8979327082633972]\n",
            "[Epoch 2/30] [Batch 53/66] [D loss: 0.000261385619523935] [G loss: 0.8298591375350952]\n",
            "[Epoch 2/30] [Batch 54/66] [D loss: 0.0002714170695981011] [G loss: 0.8335620760917664]\n",
            "[Epoch 2/30] [Batch 55/66] [D loss: 0.00026057045033667237] [G loss: 0.7862572073936462]\n",
            "[Epoch 2/30] [Batch 56/66] [D loss: 0.00026667589554563165] [G loss: 0.7880578637123108]\n",
            "[Epoch 2/30] [Batch 57/66] [D loss: 0.0002687762607820332] [G loss: 0.8272731900215149]\n",
            "[Epoch 2/30] [Batch 58/66] [D loss: 0.0002595187834231183] [G loss: 0.8293628692626953]\n",
            "[Epoch 2/30] [Batch 59/66] [D loss: 0.0002807589917210862] [G loss: 0.8192737102508545]\n",
            "[Epoch 2/30] [Batch 60/66] [D loss: 0.00030475946550723165] [G loss: 0.7284295558929443]\n",
            "[Epoch 2/30] [Batch 61/66] [D loss: 0.0002948950059362687] [G loss: 0.7348530292510986]\n",
            "[Epoch 2/30] [Batch 62/66] [D loss: 0.0002823657268891111] [G loss: 0.7515953779220581]\n",
            "[Epoch 2/30] [Batch 63/66] [D loss: 0.0002732309512794018] [G loss: 0.7527796030044556]\n",
            "[Epoch 2/30] [Batch 64/66] [D loss: 0.0002640452075866051] [G loss: 0.7514568567276001]\n",
            "[Epoch 3/30] [Batch 0/66] [D loss: 0.00026450912992004305] [G loss: 0.7831345200538635]\n",
            "[Epoch 3/30] [Batch 1/66] [D loss: 0.0002529142511775717] [G loss: 0.8573117256164551]\n",
            "[Epoch 3/30] [Batch 2/66] [D loss: 0.000254535349085927] [G loss: 0.7870631217956543]\n",
            "[Epoch 3/30] [Batch 3/66] [D loss: 0.00023661502928007394] [G loss: 0.7488086223602295]\n",
            "[Epoch 3/30] [Batch 4/66] [D loss: 0.0002402643149252981] [G loss: 0.7717036008834839]\n",
            "[Epoch 3/30] [Batch 5/66] [D loss: 0.00024428289907518774] [G loss: 0.778174102306366]\n",
            "[Epoch 3/30] [Batch 6/66] [D loss: 0.00024126024800352752] [G loss: 0.7388904690742493]\n",
            "[Epoch 3/30] [Batch 7/66] [D loss: 0.00023578194668516517] [G loss: 0.8317775726318359]\n",
            "[Epoch 3/30] [Batch 8/66] [D loss: 0.00023354555742116645] [G loss: 0.7832193374633789]\n",
            "[Epoch 3/30] [Batch 9/66] [D loss: 0.00023208244965644553] [G loss: 0.7325315475463867]\n",
            "[Epoch 3/30] [Batch 10/66] [D loss: 0.00023968172899913043] [G loss: 0.7947510480880737]\n",
            "[Epoch 3/30] [Batch 11/66] [D loss: 0.00021011825447203591] [G loss: 0.7845413088798523]\n",
            "[Epoch 3/30] [Batch 12/66] [D loss: 0.00022827587235951796] [G loss: 0.8020995855331421]\n",
            "[Epoch 3/30] [Batch 13/66] [D loss: 0.00021763825498055667] [G loss: 0.6843985915184021]\n",
            "[Epoch 3/30] [Batch 14/66] [D loss: 0.0002171793021261692] [G loss: 0.71930330991745]\n",
            "[Epoch 3/30] [Batch 15/66] [D loss: 0.00021582595218205824] [G loss: 0.6830889582633972]\n",
            "[Epoch 3/30] [Batch 16/66] [D loss: 0.00022140889632282779] [G loss: 0.7053361535072327]\n",
            "[Epoch 3/30] [Batch 17/66] [D loss: 0.0002177183487219736] [G loss: 0.7468282580375671]\n",
            "[Epoch 3/30] [Batch 18/66] [D loss: 0.00021345718414522707] [G loss: 0.7434799671173096]\n",
            "[Epoch 3/30] [Batch 19/66] [D loss: 0.00020257417781976983] [G loss: 0.6994013786315918]\n",
            "[Epoch 3/30] [Batch 20/66] [D loss: 0.0002192207393818535] [G loss: 0.7520017623901367]\n",
            "[Epoch 3/30] [Batch 21/66] [D loss: 0.00019982978119514883] [G loss: 0.8522487282752991]\n",
            "[Epoch 3/30] [Batch 22/66] [D loss: 0.00020255330309737474] [G loss: 0.7484659552574158]\n",
            "[Epoch 3/30] [Batch 23/66] [D loss: 0.00019635093485703692] [G loss: 0.8205052018165588]\n",
            "[Epoch 3/30] [Batch 24/66] [D loss: 0.00019768103811657056] [G loss: 0.7852815985679626]\n",
            "[Epoch 3/30] [Batch 25/66] [D loss: 0.00019482204515952617] [G loss: 0.7636148929595947]\n",
            "[Epoch 3/30] [Batch 26/66] [D loss: 0.00019438128219917417] [G loss: 0.7071147561073303]\n",
            "[Epoch 3/30] [Batch 27/66] [D loss: 0.00019161897216690704] [G loss: 0.6791011095046997]\n",
            "[Epoch 3/30] [Batch 28/66] [D loss: 0.00019225353753427044] [G loss: 0.6660956144332886]\n",
            "[Epoch 3/30] [Batch 29/66] [D loss: 0.00019093341688858345] [G loss: 0.7023934721946716]\n",
            "[Epoch 3/30] [Batch 30/66] [D loss: 0.00018492671370040625] [G loss: 0.6842662692070007]\n",
            "[Epoch 3/30] [Batch 31/66] [D loss: 0.00018459538841852918] [G loss: 0.728117048740387]\n",
            "[Epoch 3/30] [Batch 32/66] [D loss: 0.00019473738211672753] [G loss: 0.6940661072731018]\n",
            "[Epoch 3/30] [Batch 33/66] [D loss: 0.00018799835379468277] [G loss: 0.7028436660766602]\n",
            "[Epoch 3/30] [Batch 34/66] [D loss: 0.00018476240074960515] [G loss: 0.7185730934143066]\n",
            "[Epoch 3/30] [Batch 35/66] [D loss: 0.0001721700537018478] [G loss: 0.6806378364562988]\n",
            "[Epoch 3/30] [Batch 36/66] [D loss: 0.00017659863078733906] [G loss: 0.681267261505127]\n",
            "[Epoch 3/30] [Batch 37/66] [D loss: 0.0001821584955905564] [G loss: 0.6718204617500305]\n",
            "[Epoch 3/30] [Batch 38/66] [D loss: 0.0001735977566568181] [G loss: 0.647906482219696]\n",
            "[Epoch 3/30] [Batch 39/66] [D loss: 0.000177877489477396] [G loss: 0.7019072771072388]\n",
            "[Epoch 3/30] [Batch 40/66] [D loss: 0.00017471502360422164] [G loss: 0.6421613097190857]\n",
            "[Epoch 3/30] [Batch 41/66] [D loss: 0.00017109213513322175] [G loss: 0.663571834564209]\n",
            "[Epoch 3/30] [Batch 42/66] [D loss: 0.0001813441631384194] [G loss: 0.743447482585907]\n",
            "[Epoch 3/30] [Batch 43/66] [D loss: 0.00016700254491297528] [G loss: 0.7852548956871033]\n",
            "[Epoch 3/30] [Batch 44/66] [D loss: 0.00016891038103494793] [G loss: 0.6841918230056763]\n",
            "[Epoch 3/30] [Batch 45/66] [D loss: 0.00017310304247075692] [G loss: 0.8519531488418579]\n",
            "[Epoch 3/30] [Batch 46/66] [D loss: 0.00016024407523218542] [G loss: 0.8021126985549927]\n",
            "[Epoch 3/30] [Batch 47/66] [D loss: 0.00016696786769898608] [G loss: 0.7914462089538574]\n",
            "[Epoch 3/30] [Batch 48/66] [D loss: 0.00015563485794700682] [G loss: 0.6374274492263794]\n",
            "[Epoch 3/30] [Batch 49/66] [D loss: 0.00015111732500372455] [G loss: 0.6608144640922546]\n",
            "[Epoch 3/30] [Batch 50/66] [D loss: 0.00016209972091019154] [G loss: 0.6382302045822144]\n",
            "[Epoch 3/30] [Batch 51/66] [D loss: 0.0001579759264132008] [G loss: 0.6702419519424438]\n",
            "[Epoch 3/30] [Batch 52/66] [D loss: 0.00016153430624399334] [G loss: 0.6855960488319397]\n",
            "[Epoch 3/30] [Batch 53/66] [D loss: 0.00015708771388744935] [G loss: 0.8426855802536011]\n",
            "[Epoch 3/30] [Batch 54/66] [D loss: 0.00017153720546048135] [G loss: 0.7872565984725952]\n",
            "[Epoch 3/30] [Batch 55/66] [D loss: 0.00014898584049660712] [G loss: 0.7871001362800598]\n",
            "[Epoch 3/30] [Batch 56/66] [D loss: 0.00015522741887252778] [G loss: 0.6544915437698364]\n",
            "[Epoch 3/30] [Batch 57/66] [D loss: 0.0001602941265446134] [G loss: 0.6660162806510925]\n",
            "[Epoch 3/30] [Batch 58/66] [D loss: 0.00015178976173046976] [G loss: 0.6918992400169373]\n",
            "[Epoch 3/30] [Batch 59/66] [D loss: 0.00015010877541499212] [G loss: 0.6793431043624878]\n",
            "[Epoch 3/30] [Batch 60/66] [D loss: 0.00015382843412226066] [G loss: 0.6268571615219116]\n",
            "[Epoch 3/30] [Batch 61/66] [D loss: 0.00015194202569546178] [G loss: 0.6626696586608887]\n",
            "[Epoch 3/30] [Batch 62/66] [D loss: 0.00015378952957689762] [G loss: 0.7110753655433655]\n",
            "[Epoch 3/30] [Batch 63/66] [D loss: 0.00014186831685947254] [G loss: 0.6563296318054199]\n",
            "[Epoch 3/30] [Batch 64/66] [D loss: 0.00014970260963309556] [G loss: 0.6698513627052307]\n",
            "[Epoch 4/30] [Batch 0/66] [D loss: 0.0001518173303338699] [G loss: 0.6926184892654419]\n",
            "[Epoch 4/30] [Batch 1/66] [D loss: 0.0001514798277639784] [G loss: 0.6749016642570496]\n",
            "[Epoch 4/30] [Batch 2/66] [D loss: 0.00014562709839083254] [G loss: 0.6574391722679138]\n",
            "[Epoch 4/30] [Batch 3/66] [D loss: 0.00014531709894072264] [G loss: 0.6224937438964844]\n",
            "[Epoch 4/30] [Batch 4/66] [D loss: 0.0001441873755538836] [G loss: 0.6908838152885437]\n",
            "[Epoch 4/30] [Batch 5/66] [D loss: 0.0001373550039716065] [G loss: 0.6395781636238098]\n",
            "[Epoch 4/30] [Batch 6/66] [D loss: 0.00014184384053805843] [G loss: 0.6188063621520996]\n",
            "[Epoch 4/30] [Batch 7/66] [D loss: 0.00013696689711650833] [G loss: 0.5858789682388306]\n",
            "[Epoch 4/30] [Batch 8/66] [D loss: 0.0001464270389988087] [G loss: 0.8605973720550537]\n",
            "[Epoch 4/30] [Batch 9/66] [D loss: 0.00012991111725568771] [G loss: 0.6798466444015503]\n",
            "[Epoch 4/30] [Batch 10/66] [D loss: 0.00013700014824280515] [G loss: 0.6435360312461853]\n",
            "[Epoch 4/30] [Batch 11/66] [D loss: 0.0001373796840198338] [G loss: 0.6692920923233032]\n",
            "[Epoch 4/30] [Batch 12/66] [D loss: 0.00013020528422202915] [G loss: 0.595733106136322]\n",
            "[Epoch 4/30] [Batch 13/66] [D loss: 0.00013081519136903808] [G loss: 0.6183868050575256]\n",
            "[Epoch 4/30] [Batch 14/66] [D loss: 0.00013465187657857314] [G loss: 0.678687334060669]\n",
            "[Epoch 4/30] [Batch 15/66] [D loss: 0.000133592177007813] [G loss: 0.606224536895752]\n",
            "[Epoch 4/30] [Batch 16/66] [D loss: 0.00013929951091995463] [G loss: 0.6448113322257996]\n",
            "[Epoch 4/30] [Batch 17/66] [D loss: 0.00013239835971035063] [G loss: 0.6334304213523865]\n",
            "[Epoch 4/30] [Batch 18/66] [D loss: 0.00013212992780609056] [G loss: 0.6030085682868958]\n",
            "[Epoch 4/30] [Batch 19/66] [D loss: 0.0001310301959165372] [G loss: 0.6203051805496216]\n",
            "[Epoch 4/30] [Batch 20/66] [D loss: 0.00013093752204440534] [G loss: 0.6245555877685547]\n",
            "[Epoch 4/30] [Batch 21/66] [D loss: 0.0001346447825198993] [G loss: 0.616646409034729]\n",
            "[Epoch 4/30] [Batch 22/66] [D loss: 0.00013660835975315422] [G loss: 0.6483113765716553]\n",
            "[Epoch 4/30] [Batch 23/66] [D loss: 0.00012098221486667171] [G loss: 0.5580015182495117]\n",
            "[Epoch 4/30] [Batch 24/66] [D loss: 0.00012422392319422215] [G loss: 0.6088794469833374]\n",
            "[Epoch 4/30] [Batch 25/66] [D loss: 0.00012172874994575977] [G loss: 0.5864810943603516]\n",
            "[Epoch 4/30] [Batch 26/66] [D loss: 0.00012936616985825822] [G loss: 0.6382033228874207]\n",
            "[Epoch 4/30] [Batch 27/66] [D loss: 0.00012701062951236963] [G loss: 0.6747977137565613]\n",
            "[Epoch 4/30] [Batch 28/66] [D loss: 0.00012821731070289388] [G loss: 0.6175844073295593]\n",
            "[Epoch 4/30] [Batch 29/66] [D loss: 0.00012333712220424786] [G loss: 0.6013073325157166]\n",
            "[Epoch 4/30] [Batch 30/66] [D loss: 0.00012478386634029448] [G loss: 0.6001548767089844]\n",
            "[Epoch 4/30] [Batch 31/66] [D loss: 0.0001189888353110291] [G loss: 0.6384917497634888]\n",
            "[Epoch 4/30] [Batch 32/66] [D loss: 0.00012771676847478375] [G loss: 0.6865936517715454]\n",
            "[Epoch 4/30] [Batch 33/66] [D loss: 0.00011546158202691004] [G loss: 0.623670220375061]\n",
            "[Epoch 4/30] [Batch 34/66] [D loss: 0.00011836249541374855] [G loss: 0.611272394657135]\n",
            "[Epoch 4/30] [Batch 35/66] [D loss: 0.00011541638377821073] [G loss: 0.6169472932815552]\n",
            "[Epoch 4/30] [Batch 36/66] [D loss: 0.00011472104233689606] [G loss: 0.6841667890548706]\n",
            "[Epoch 4/30] [Batch 37/66] [D loss: 0.00011754195293178782] [G loss: 0.5890815258026123]\n",
            "[Epoch 4/30] [Batch 38/66] [D loss: 0.00011433576219133101] [G loss: 0.6261340975761414]\n",
            "[Epoch 4/30] [Batch 39/66] [D loss: 0.00011231070311623625] [G loss: 0.6414934992790222]\n",
            "[Epoch 4/30] [Batch 40/66] [D loss: 0.00010816957365022972] [G loss: 0.6850230097770691]\n",
            "[Epoch 4/30] [Batch 41/66] [D loss: 0.00011761577115976252] [G loss: 0.6284244060516357]\n",
            "[Epoch 4/30] [Batch 42/66] [D loss: 0.00011027084110537544] [G loss: 0.591657280921936]\n",
            "[Epoch 4/30] [Batch 43/66] [D loss: 0.00011102386633865535] [G loss: 0.5900357961654663]\n",
            "[Epoch 4/30] [Batch 44/66] [D loss: 0.00011051560795749538] [G loss: 0.7347373962402344]\n",
            "[Epoch 4/30] [Batch 45/66] [D loss: 0.00011184281174791977] [G loss: 0.6743056774139404]\n",
            "[Epoch 4/30] [Batch 46/66] [D loss: 0.00011049730164813809] [G loss: 0.5968129634857178]\n",
            "[Epoch 4/30] [Batch 47/66] [D loss: 0.000110663466330152] [G loss: 0.7406134605407715]\n",
            "[Epoch 4/30] [Batch 48/66] [D loss: 0.0001227712054969743] [G loss: 0.7345911860466003]\n",
            "[Epoch 4/30] [Batch 49/66] [D loss: 0.00010926461982307956] [G loss: 0.5871638059616089]\n",
            "[Epoch 4/30] [Batch 50/66] [D loss: 0.00010531623047427274] [G loss: 0.6345566511154175]\n",
            "[Epoch 4/30] [Batch 51/66] [D loss: 0.00011015686686732806] [G loss: 0.6058546304702759]\n",
            "[Epoch 4/30] [Batch 52/66] [D loss: 0.00010834071144927293] [G loss: 0.5968526601791382]\n",
            "[Epoch 4/30] [Batch 53/66] [D loss: 0.00010777335410239175] [G loss: 0.5785938501358032]\n",
            "[Epoch 4/30] [Batch 54/66] [D loss: 0.0001110083467210643] [G loss: 0.6215921640396118]\n",
            "[Epoch 4/30] [Batch 55/66] [D loss: 0.00010525627294555306] [G loss: 0.5638960599899292]\n",
            "[Epoch 4/30] [Batch 56/66] [D loss: 0.00010171095345867798] [G loss: 0.5942885875701904]\n",
            "[Epoch 4/30] [Batch 57/66] [D loss: 0.00010691418356145732] [G loss: 0.6212760210037231]\n",
            "[Epoch 4/30] [Batch 58/66] [D loss: 0.00010238533286610618] [G loss: 0.5614892244338989]\n",
            "[Epoch 4/30] [Batch 59/66] [D loss: 0.00010272516738041304] [G loss: 0.5402916073799133]\n",
            "[Epoch 4/30] [Batch 60/66] [D loss: 0.00010290597128914669] [G loss: 0.5682628750801086]\n",
            "[Epoch 4/30] [Batch 61/66] [D loss: 0.00010293257219018415] [G loss: 0.6440286636352539]\n",
            "[Epoch 4/30] [Batch 62/66] [D loss: 0.00010224212019238621] [G loss: 0.6150047183036804]\n",
            "[Epoch 4/30] [Batch 63/66] [D loss: 9.70264918578323e-05] [G loss: 0.5312398672103882]\n",
            "[Epoch 4/30] [Batch 64/66] [D loss: 0.00010385950008640066] [G loss: 0.558213472366333]\n",
            "[Epoch 5/30] [Batch 0/66] [D loss: 0.00010981145896948874] [G loss: 0.7123929262161255]\n",
            "[Epoch 5/30] [Batch 1/66] [D loss: 0.00010204888531006873] [G loss: 0.5774086713790894]\n",
            "[Epoch 5/30] [Batch 2/66] [D loss: 9.918447176460177e-05] [G loss: 0.6047188639640808]\n",
            "[Epoch 5/30] [Batch 3/66] [D loss: 9.935426351148635e-05] [G loss: 0.6347800493240356]\n",
            "[Epoch 5/30] [Batch 4/66] [D loss: 9.881638834485784e-05] [G loss: 0.6085514426231384]\n",
            "[Epoch 5/30] [Batch 5/66] [D loss: 9.59141761995852e-05] [G loss: 0.6206036806106567]\n",
            "[Epoch 5/30] [Batch 6/66] [D loss: 0.00010012362690758891] [G loss: 0.6510787010192871]\n",
            "[Epoch 5/30] [Batch 7/66] [D loss: 9.544749627821147e-05] [G loss: 0.716570258140564]\n",
            "[Epoch 5/30] [Batch 8/66] [D loss: 9.840367420110852e-05] [G loss: 0.5410342216491699]\n",
            "[Epoch 5/30] [Batch 9/66] [D loss: 9.546786168357357e-05] [G loss: 0.596627950668335]\n",
            "[Epoch 5/30] [Batch 10/66] [D loss: 9.671620136941783e-05] [G loss: 0.6252928972244263]\n",
            "[Epoch 5/30] [Batch 11/66] [D loss: 9.37714139581658e-05] [G loss: 0.5895320773124695]\n",
            "[Epoch 5/30] [Batch 12/66] [D loss: 9.635377500671893e-05] [G loss: 0.5811301469802856]\n",
            "[Epoch 5/30] [Batch 13/66] [D loss: 9.583482096786611e-05] [G loss: 0.6075339317321777]\n",
            "[Epoch 5/30] [Batch 14/66] [D loss: 9.56674775807187e-05] [G loss: 0.585699737071991]\n",
            "[Epoch 5/30] [Batch 15/66] [D loss: 9.233879973180592e-05] [G loss: 0.5427448749542236]\n",
            "[Epoch 5/30] [Batch 16/66] [D loss: 9.209852214553393e-05] [G loss: 0.5565646886825562]\n",
            "[Epoch 5/30] [Batch 17/66] [D loss: 9.228839917341247e-05] [G loss: 0.684421956539154]\n",
            "[Epoch 5/30] [Batch 18/66] [D loss: 9.458120621275157e-05] [G loss: 0.5344383716583252]\n",
            "[Epoch 5/30] [Batch 19/66] [D loss: 9.515150304650888e-05] [G loss: 0.6010781526565552]\n",
            "[Epoch 5/30] [Batch 20/66] [D loss: 9.435321771888994e-05] [G loss: 0.5768740177154541]\n",
            "[Epoch 5/30] [Batch 21/66] [D loss: 8.889613673090935e-05] [G loss: 0.554996132850647]\n",
            "[Epoch 5/30] [Batch 22/66] [D loss: 8.987418914330192e-05] [G loss: 0.5426238179206848]\n",
            "[Epoch 5/30] [Batch 23/66] [D loss: 8.908594463719055e-05] [G loss: 0.570014238357544]\n",
            "[Epoch 5/30] [Batch 24/66] [D loss: 9.445975592825562e-05] [G loss: 0.7142625451087952]\n",
            "[Epoch 5/30] [Batch 25/66] [D loss: 8.536014502169564e-05] [G loss: 0.62944096326828]\n",
            "[Epoch 5/30] [Batch 26/66] [D loss: 9.241227962775156e-05] [G loss: 0.7067010402679443]\n",
            "[Epoch 5/30] [Batch 27/66] [D loss: 9.679057257017121e-05] [G loss: 0.6384121775627136]\n",
            "[Epoch 5/30] [Batch 28/66] [D loss: 8.883577174856327e-05] [G loss: 0.543259859085083]\n",
            "[Epoch 5/30] [Batch 29/66] [D loss: 8.72081182023976e-05] [G loss: 0.5560309290885925]\n",
            "[Epoch 5/30] [Batch 30/66] [D loss: 8.869662633514963e-05] [G loss: 0.5572461485862732]\n",
            "[Epoch 5/30] [Batch 31/66] [D loss: 8.989383786683902e-05] [G loss: 0.6574671268463135]\n",
            "[Epoch 5/30] [Batch 32/66] [D loss: 8.593365419073962e-05] [G loss: 0.5388159155845642]\n",
            "[Epoch 5/30] [Batch 33/66] [D loss: 8.626294584246352e-05] [G loss: 0.5221025943756104]\n",
            "[Epoch 5/30] [Batch 34/66] [D loss: 8.542693831259385e-05] [G loss: 0.633317232131958]\n",
            "[Epoch 5/30] [Batch 35/66] [D loss: 8.303670620080084e-05] [G loss: 0.5332660675048828]\n",
            "[Epoch 5/30] [Batch 36/66] [D loss: 8.640076703159139e-05] [G loss: 0.5177372097969055]\n",
            "[Epoch 5/30] [Batch 37/66] [D loss: 8.575295942137018e-05] [G loss: 0.5866693258285522]\n",
            "[Epoch 5/30] [Batch 38/66] [D loss: 8.201228411053307e-05] [G loss: 0.5660306215286255]\n",
            "[Epoch 5/30] [Batch 39/66] [D loss: 8.429219451500103e-05] [G loss: 0.510326087474823]\n",
            "[Epoch 5/30] [Batch 40/66] [D loss: 8.541450733901002e-05] [G loss: 0.578615128993988]\n",
            "[Epoch 5/30] [Batch 41/66] [D loss: 8.353944213013165e-05] [G loss: 0.6384037733078003]\n",
            "[Epoch 5/30] [Batch 42/66] [D loss: 8.442873149760999e-05] [G loss: 0.577939510345459]\n",
            "[Epoch 5/30] [Batch 43/66] [D loss: 8.087101741693914e-05] [G loss: 0.5979601144790649]\n",
            "[Epoch 5/30] [Batch 44/66] [D loss: 8.365189569303766e-05] [G loss: 0.5146718621253967]\n",
            "[Epoch 5/30] [Batch 45/66] [D loss: 8.339305713889189e-05] [G loss: 0.5462383031845093]\n",
            "[Epoch 5/30] [Batch 46/66] [D loss: 8.166575935319997e-05] [G loss: 0.5609714388847351]\n",
            "[Epoch 5/30] [Batch 47/66] [D loss: 8.011210229597054e-05] [G loss: 0.518258273601532]\n",
            "[Epoch 5/30] [Batch 48/66] [D loss: 7.84352705522906e-05] [G loss: 0.5906118750572205]\n",
            "[Epoch 5/30] [Batch 49/66] [D loss: 8.35394203022588e-05] [G loss: 0.564672589302063]\n",
            "[Epoch 5/30] [Batch 50/66] [D loss: 8.038594751269557e-05] [G loss: 0.5497528314590454]\n",
            "[Epoch 5/30] [Batch 51/66] [D loss: 8.044731293921359e-05] [G loss: 0.5980820655822754]\n",
            "[Epoch 5/30] [Batch 52/66] [D loss: 8.588733180658892e-05] [G loss: 0.5501385927200317]\n",
            "[Epoch 5/30] [Batch 53/66] [D loss: 8.122893996187486e-05] [G loss: 0.5990236401557922]\n",
            "[Epoch 5/30] [Batch 54/66] [D loss: 7.814677519490942e-05] [G loss: 0.6069439649581909]\n",
            "[Epoch 5/30] [Batch 55/66] [D loss: 8.011230966076255e-05] [G loss: 0.5440541505813599]\n",
            "[Epoch 5/30] [Batch 56/66] [D loss: 7.937733607832342e-05] [G loss: 0.5668986439704895]\n",
            "[Epoch 5/30] [Batch 57/66] [D loss: 7.674993321415968e-05] [G loss: 0.5749270915985107]\n",
            "[Epoch 5/30] [Batch 58/66] [D loss: 7.768203795421869e-05] [G loss: 0.5230695605278015]\n",
            "[Epoch 5/30] [Batch 59/66] [D loss: 7.645776713616215e-05] [G loss: 0.5331828594207764]\n",
            "[Epoch 5/30] [Batch 60/66] [D loss: 7.609424210386351e-05] [G loss: 0.5692081451416016]\n",
            "[Epoch 5/30] [Batch 61/66] [D loss: 7.737598207313567e-05] [G loss: 0.5405439734458923]\n",
            "[Epoch 5/30] [Batch 62/66] [D loss: 7.483734952984378e-05] [G loss: 0.5623490810394287]\n",
            "[Epoch 5/30] [Batch 63/66] [D loss: 7.559893128927797e-05] [G loss: 0.6444574594497681]\n",
            "[Epoch 5/30] [Batch 64/66] [D loss: 7.427663513226435e-05] [G loss: 0.5635837912559509]\n",
            "[Epoch 6/30] [Batch 0/66] [D loss: 7.56597328290809e-05] [G loss: 0.5158578753471375]\n",
            "[Epoch 6/30] [Batch 1/66] [D loss: 7.427793025271967e-05] [G loss: 0.5329480767250061]\n",
            "[Epoch 6/30] [Batch 2/66] [D loss: 7.384451600955799e-05] [G loss: 0.5328937768936157]\n",
            "[Epoch 6/30] [Batch 3/66] [D loss: 7.369602826656774e-05] [G loss: 0.5166193842887878]\n",
            "[Epoch 6/30] [Batch 4/66] [D loss: 7.570153320557438e-05] [G loss: 0.5143240690231323]\n",
            "[Epoch 6/30] [Batch 5/66] [D loss: 7.758232823107392e-05] [G loss: 0.5221201181411743]\n",
            "[Epoch 6/30] [Batch 6/66] [D loss: 7.40627947379835e-05] [G loss: 0.5219379663467407]\n",
            "[Epoch 6/30] [Batch 7/66] [D loss: 7.131163874873891e-05] [G loss: 0.6328749060630798]\n",
            "[Epoch 6/30] [Batch 8/66] [D loss: 7.59063841542229e-05] [G loss: 0.5996637344360352]\n",
            "[Epoch 6/30] [Batch 9/66] [D loss: 7.432009442709386e-05] [G loss: 0.5360564589500427]\n",
            "[Epoch 6/30] [Batch 10/66] [D loss: 7.208584793261252e-05] [G loss: 0.5120301246643066]\n",
            "[Epoch 6/30] [Batch 11/66] [D loss: 7.526425179094076e-05] [G loss: 0.5888656377792358]\n",
            "[Epoch 6/30] [Batch 12/66] [D loss: 7.590620589326136e-05] [G loss: 0.5657516717910767]\n",
            "[Epoch 6/30] [Batch 13/66] [D loss: 7.116172855603509e-05] [G loss: 0.5043042898178101]\n",
            "[Epoch 6/30] [Batch 14/66] [D loss: 7.053821173030883e-05] [G loss: 0.5511044859886169]\n",
            "[Epoch 6/30] [Batch 15/66] [D loss: 7.044376980047673e-05] [G loss: 0.5392684936523438]\n",
            "[Epoch 6/30] [Batch 16/66] [D loss: 7.202631240943447e-05] [G loss: 0.5111356973648071]\n",
            "[Epoch 6/30] [Batch 17/66] [D loss: 7.097113120835274e-05] [G loss: 0.5266525745391846]\n",
            "[Epoch 6/30] [Batch 18/66] [D loss: 7.169728996814229e-05] [G loss: 0.5704338550567627]\n",
            "[Epoch 6/30] [Batch 19/66] [D loss: 6.950898750801571e-05] [G loss: 0.4862363636493683]\n",
            "[Epoch 6/30] [Batch 20/66] [D loss: 6.837135879322886e-05] [G loss: 0.5148486495018005]\n",
            "[Epoch 6/30] [Batch 21/66] [D loss: 6.820339331170544e-05] [G loss: 0.4884990453720093]\n",
            "[Epoch 6/30] [Batch 22/66] [D loss: 6.84720289427787e-05] [G loss: 0.592181921005249]\n",
            "[Epoch 6/30] [Batch 23/66] [D loss: 6.700946323690005e-05] [G loss: 0.5080849528312683]\n",
            "[Epoch 6/30] [Batch 24/66] [D loss: 6.930535164428875e-05] [G loss: 0.5290408730506897]\n",
            "[Epoch 6/30] [Batch 25/66] [D loss: 6.655859033344314e-05] [G loss: 0.5117356777191162]\n",
            "[Epoch 6/30] [Batch 26/66] [D loss: 6.951284376555122e-05] [G loss: 0.5042699575424194]\n",
            "[Epoch 6/30] [Batch 27/66] [D loss: 7.194669888122007e-05] [G loss: 0.5233495235443115]\n",
            "[Epoch 6/30] [Batch 28/66] [D loss: 6.97636351105757e-05] [G loss: 0.5355736017227173]\n",
            "[Epoch 6/30] [Batch 29/66] [D loss: 6.547902376041748e-05] [G loss: 0.5089333057403564]\n",
            "[Epoch 6/30] [Batch 30/66] [D loss: 6.722626494592987e-05] [G loss: 0.5450472831726074]\n",
            "[Epoch 6/30] [Batch 31/66] [D loss: 6.948730151634663e-05] [G loss: 0.5398730039596558]\n",
            "[Epoch 6/30] [Batch 32/66] [D loss: 6.356402082019486e-05] [G loss: 0.5234681367874146]\n",
            "[Epoch 6/30] [Batch 33/66] [D loss: 6.72939422656782e-05] [G loss: 0.5452353954315186]\n",
            "[Epoch 6/30] [Batch 34/66] [D loss: 6.25869033683557e-05] [G loss: 0.48457807302474976]\n",
            "[Epoch 6/30] [Batch 35/66] [D loss: 6.381225102813914e-05] [G loss: 0.4833650588989258]\n",
            "[Epoch 6/30] [Batch 36/66] [D loss: 6.531368489959277e-05] [G loss: 0.49200642108917236]\n",
            "[Epoch 6/30] [Batch 37/66] [D loss: 6.609327101614326e-05] [G loss: 0.5359283685684204]\n",
            "[Epoch 6/30] [Batch 38/66] [D loss: 6.543305062223226e-05] [G loss: 0.4780536890029907]\n",
            "[Epoch 6/30] [Batch 39/66] [D loss: 6.501205280073918e-05] [G loss: 0.5592753887176514]\n",
            "[Epoch 6/30] [Batch 40/66] [D loss: 6.204197597980965e-05] [G loss: 0.6592870354652405]\n",
            "[Epoch 6/30] [Batch 41/66] [D loss: 6.571897029061802e-05] [G loss: 0.4906485676765442]\n",
            "[Epoch 6/30] [Batch 42/66] [D loss: 6.464072794187814e-05] [G loss: 0.5781052112579346]\n",
            "[Epoch 6/30] [Batch 43/66] [D loss: 6.126900188974105e-05] [G loss: 0.5327236652374268]\n",
            "[Epoch 6/30] [Batch 44/66] [D loss: 6.494758417829871e-05] [G loss: 0.6305570602416992]\n",
            "[Epoch 6/30] [Batch 45/66] [D loss: 6.37164557701908e-05] [G loss: 0.5231552124023438]\n",
            "[Epoch 6/30] [Batch 46/66] [D loss: 6.288392978603952e-05] [G loss: 0.48728829622268677]\n",
            "[Epoch 6/30] [Batch 47/66] [D loss: 6.352104901452549e-05] [G loss: 0.5080102682113647]\n",
            "[Epoch 6/30] [Batch 48/66] [D loss: 6.332705743261613e-05] [G loss: 0.5125531554222107]\n",
            "[Epoch 6/30] [Batch 49/66] [D loss: 6.0848477005492896e-05] [G loss: 0.592017412185669]\n",
            "[Epoch 6/30] [Batch 50/66] [D loss: 6.17555451754015e-05] [G loss: 0.4622310400009155]\n",
            "[Epoch 6/30] [Batch 51/66] [D loss: 6.141109952295665e-05] [G loss: 0.49751678109169006]\n",
            "[Epoch 6/30] [Batch 52/66] [D loss: 5.973864972474985e-05] [G loss: 0.47584980726242065]\n",
            "[Epoch 6/30] [Batch 53/66] [D loss: 6.235747241589706e-05] [G loss: 0.4844558835029602]\n",
            "[Epoch 6/30] [Batch 54/66] [D loss: 6.103302075644024e-05] [G loss: 0.5077885389328003]\n",
            "[Epoch 6/30] [Batch 55/66] [D loss: 6.156462222861592e-05] [G loss: 0.5237005949020386]\n",
            "[Epoch 6/30] [Batch 56/66] [D loss: 6.12128060311079e-05] [G loss: 0.583294153213501]\n",
            "[Epoch 6/30] [Batch 57/66] [D loss: 5.9023564972449094e-05] [G loss: 0.5403170585632324]\n",
            "[Epoch 6/30] [Batch 58/66] [D loss: 5.926883750362322e-05] [G loss: 0.5016964673995972]\n",
            "[Epoch 6/30] [Batch 59/66] [D loss: 6.152279638627078e-05] [G loss: 0.5016518235206604]\n",
            "[Epoch 6/30] [Batch 60/66] [D loss: 6.164564911159687e-05] [G loss: 0.4675910472869873]\n",
            "[Epoch 6/30] [Batch 61/66] [D loss: 6.154395487101283e-05] [G loss: 0.511880099773407]\n",
            "[Epoch 6/30] [Batch 62/66] [D loss: 5.8038614952238277e-05] [G loss: 0.45487967133522034]\n",
            "[Epoch 6/30] [Batch 63/66] [D loss: 5.906630394747481e-05] [G loss: 0.5804384350776672]\n",
            "[Epoch 6/30] [Batch 64/66] [D loss: 5.755287202191539e-05] [G loss: 0.5119113922119141]\n",
            "[Epoch 7/30] [Batch 0/66] [D loss: 5.726336166844703e-05] [G loss: 0.46722647547721863]\n",
            "[Epoch 7/30] [Batch 1/66] [D loss: 6.0418395150918514e-05] [G loss: 0.4802291989326477]\n",
            "[Epoch 7/30] [Batch 2/66] [D loss: 5.831206544826273e-05] [G loss: 0.46386319398880005]\n",
            "[Epoch 7/30] [Batch 3/66] [D loss: 5.748478906753007e-05] [G loss: 0.4791550040245056]\n",
            "[Epoch 7/30] [Batch 4/66] [D loss: 5.8733023251988925e-05] [G loss: 0.5231848359107971]\n",
            "[Epoch 7/30] [Batch 5/66] [D loss: 5.9114872783538885e-05] [G loss: 0.500458836555481]\n",
            "[Epoch 7/30] [Batch 6/66] [D loss: 5.906081787543371e-05] [G loss: 0.5274800062179565]\n",
            "[Epoch 7/30] [Batch 7/66] [D loss: 5.821602826472372e-05] [G loss: 0.5646609663963318]\n",
            "[Epoch 7/30] [Batch 8/66] [D loss: 5.708397111447994e-05] [G loss: 0.47108834981918335]\n",
            "[Epoch 7/30] [Batch 9/66] [D loss: 5.4903310228837654e-05] [G loss: 0.5031957626342773]\n",
            "[Epoch 7/30] [Batch 10/66] [D loss: 5.577452429861296e-05] [G loss: 0.48263347148895264]\n",
            "[Epoch 7/30] [Batch 11/66] [D loss: 5.7225483033107594e-05] [G loss: 0.532268762588501]\n",
            "[Epoch 7/30] [Batch 12/66] [D loss: 5.54788057343103e-05] [G loss: 0.48841413855552673]\n",
            "[Epoch 7/30] [Batch 13/66] [D loss: 5.442785004561301e-05] [G loss: 0.550473690032959]\n",
            "[Epoch 7/30] [Batch 14/66] [D loss: 5.502728890860453e-05] [G loss: 0.6040526628494263]\n",
            "[Epoch 7/30] [Batch 15/66] [D loss: 5.681481343344785e-05] [G loss: 0.517939567565918]\n",
            "[Epoch 7/30] [Batch 16/66] [D loss: 5.5724613048369065e-05] [G loss: 0.48355987668037415]\n",
            "[Epoch 7/30] [Batch 17/66] [D loss: 5.5398659242200665e-05] [G loss: 0.5282200574874878]\n",
            "[Epoch 7/30] [Batch 18/66] [D loss: 5.6601656979182735e-05] [G loss: 0.5049100518226624]\n",
            "[Epoch 7/30] [Batch 19/66] [D loss: 5.550903188122902e-05] [G loss: 0.5951933264732361]\n",
            "[Epoch 7/30] [Batch 20/66] [D loss: 5.5651060392847285e-05] [G loss: 0.49743127822875977]\n",
            "[Epoch 7/30] [Batch 21/66] [D loss: 5.622865501209162e-05] [G loss: 0.5380223989486694]\n",
            "[Epoch 7/30] [Batch 22/66] [D loss: 5.483563472807873e-05] [G loss: 0.656104326248169]\n",
            "[Epoch 7/30] [Batch 23/66] [D loss: 5.5288561270572245e-05] [G loss: 0.557485818862915]\n",
            "[Epoch 7/30] [Batch 24/66] [D loss: 5.506498200702481e-05] [G loss: 0.5763042569160461]\n",
            "[Epoch 7/30] [Batch 25/66] [D loss: 5.4638720030197874e-05] [G loss: 0.6738296747207642]\n",
            "[Epoch 7/30] [Batch 26/66] [D loss: 5.6959350331453606e-05] [G loss: 0.6030220985412598]\n",
            "[Epoch 7/30] [Batch 27/66] [D loss: 5.269761459203437e-05] [G loss: 0.6619073152542114]\n",
            "[Epoch 7/30] [Batch 28/66] [D loss: 8.992760012915824e-05] [G loss: 0.7288362383842468]\n",
            "[Epoch 7/30] [Batch 29/66] [D loss: 6.879129614389967e-05] [G loss: 0.6231558322906494]\n",
            "[Epoch 7/30] [Batch 30/66] [D loss: 8.806392361293547e-05] [G loss: 0.6481771469116211]\n",
            "[Epoch 7/30] [Batch 31/66] [D loss: 6.8169927544659e-05] [G loss: 0.6230990886688232]\n",
            "[Epoch 7/30] [Batch 32/66] [D loss: 6.339600440696813e-05] [G loss: 0.6154465079307556]\n",
            "[Epoch 7/30] [Batch 33/66] [D loss: 5.901047188672237e-05] [G loss: 0.6251898407936096]\n",
            "[Epoch 7/30] [Batch 34/66] [D loss: 6.315439168247394e-05] [G loss: 0.5827703475952148]\n",
            "[Epoch 7/30] [Batch 35/66] [D loss: 5.345438694348559e-05] [G loss: 0.585082471370697]\n",
            "[Epoch 7/30] [Batch 36/66] [D loss: 5.4647442084387876e-05] [G loss: 0.5555177927017212]\n",
            "[Epoch 7/30] [Batch 37/66] [D loss: 5.7509285397827625e-05] [G loss: 0.5410167574882507]\n",
            "[Epoch 7/30] [Batch 38/66] [D loss: 5.5461834563175216e-05] [G loss: 0.5273355841636658]\n",
            "[Epoch 7/30] [Batch 39/66] [D loss: 5.394427353166975e-05] [G loss: 0.48537373542785645]\n",
            "[Epoch 7/30] [Batch 40/66] [D loss: 5.564765160670504e-05] [G loss: 0.5159299969673157]\n",
            "[Epoch 7/30] [Batch 41/66] [D loss: 5.4422624089056626e-05] [G loss: 0.5512409806251526]\n",
            "[Epoch 7/30] [Batch 42/66] [D loss: 5.3465599194169044e-05] [G loss: 0.5915768146514893]\n",
            "[Epoch 7/30] [Batch 43/66] [D loss: 5.499273174791597e-05] [G loss: 0.4663540720939636]\n",
            "[Epoch 7/30] [Batch 44/66] [D loss: 5.415227496996522e-05] [G loss: 0.515654444694519]\n",
            "[Epoch 7/30] [Batch 45/66] [D loss: 5.3745014156447724e-05] [G loss: 0.5296944379806519]\n",
            "[Epoch 7/30] [Batch 46/66] [D loss: 5.075615808891598e-05] [G loss: 0.6588834524154663]\n",
            "[Epoch 7/30] [Batch 47/66] [D loss: 5.0482889491831884e-05] [G loss: 0.5193649530410767]\n",
            "[Epoch 7/30] [Batch 48/66] [D loss: 5.1151233492419124e-05] [G loss: 0.5066181421279907]\n",
            "[Epoch 7/30] [Batch 49/66] [D loss: 5.490972580446396e-05] [G loss: 0.5648109912872314]\n",
            "[Epoch 7/30] [Batch 50/66] [D loss: 5.2390023483894765e-05] [G loss: 0.521061897277832]\n",
            "[Epoch 7/30] [Batch 51/66] [D loss: 5.280201912682969e-05] [G loss: 0.5185758471488953]\n",
            "[Epoch 7/30] [Batch 52/66] [D loss: 5.055961992184166e-05] [G loss: 0.6011597514152527]\n",
            "[Epoch 7/30] [Batch 53/66] [D loss: 5.221079663897399e-05] [G loss: 0.5176019072532654]\n",
            "[Epoch 7/30] [Batch 54/66] [D loss: 5.3289233619580045e-05] [G loss: 0.5464664101600647]\n",
            "[Epoch 7/30] [Batch 55/66] [D loss: 5.050201252743136e-05] [G loss: 0.5616818070411682]\n",
            "[Epoch 7/30] [Batch 56/66] [D loss: 5.0589458624017425e-05] [G loss: 0.4617689251899719]\n",
            "[Epoch 7/30] [Batch 57/66] [D loss: 4.9704971388564445e-05] [G loss: 0.5655231475830078]\n",
            "[Epoch 7/30] [Batch 58/66] [D loss: 5.192320895730518e-05] [G loss: 0.4899943470954895]\n",
            "[Epoch 7/30] [Batch 59/66] [D loss: 5.08778484800132e-05] [G loss: 0.49864763021469116]\n",
            "[Epoch 7/30] [Batch 60/66] [D loss: 4.806812466995325e-05] [G loss: 0.4887765645980835]\n",
            "[Epoch 7/30] [Batch 61/66] [D loss: 5.1962198995170183e-05] [G loss: 0.4864211976528168]\n",
            "[Epoch 7/30] [Batch 62/66] [D loss: 4.989513945474755e-05] [G loss: 0.4793684482574463]\n",
            "[Epoch 7/30] [Batch 63/66] [D loss: 4.935501783620566e-05] [G loss: 0.5781320929527283]\n",
            "[Epoch 7/30] [Batch 64/66] [D loss: 4.921097934129648e-05] [G loss: 0.4709830582141876]\n",
            "[Epoch 8/30] [Batch 0/66] [D loss: 4.801927934749983e-05] [G loss: 0.549190878868103]\n",
            "[Epoch 8/30] [Batch 1/66] [D loss: 5.015029455535114e-05] [G loss: 0.5094415545463562]\n",
            "[Epoch 8/30] [Batch 2/66] [D loss: 4.8130168579518795e-05] [G loss: 0.4978852868080139]\n",
            "[Epoch 8/30] [Batch 3/66] [D loss: 4.717207593785133e-05] [G loss: 0.4482518434524536]\n",
            "[Epoch 8/30] [Batch 4/66] [D loss: 4.7064626414794475e-05] [G loss: 0.47979259490966797]\n",
            "[Epoch 8/30] [Batch 5/66] [D loss: 4.6896704589016736e-05] [G loss: 0.45053592324256897]\n",
            "[Epoch 8/30] [Batch 6/66] [D loss: 4.723879283119459e-05] [G loss: 0.46824896335601807]\n",
            "[Epoch 8/30] [Batch 7/66] [D loss: 4.814341627934482e-05] [G loss: 0.5371590852737427]\n",
            "[Epoch 8/30] [Batch 8/66] [D loss: 4.518420246313326e-05] [G loss: 0.4562559127807617]\n",
            "[Epoch 8/30] [Batch 9/66] [D loss: 4.830446596315596e-05] [G loss: 0.4833097755908966]\n",
            "[Epoch 8/30] [Batch 10/66] [D loss: 4.672865725297015e-05] [G loss: 0.45311495661735535]\n",
            "[Epoch 8/30] [Batch 11/66] [D loss: 4.681127938965801e-05] [G loss: 0.4593936800956726]\n",
            "[Epoch 8/30] [Batch 12/66] [D loss: 4.529506986727938e-05] [G loss: 0.4743167757987976]\n",
            "[Epoch 8/30] [Batch 13/66] [D loss: 4.4732809328706935e-05] [G loss: 0.49994704127311707]\n",
            "[Epoch 8/30] [Batch 14/66] [D loss: 4.7927302148309536e-05] [G loss: 0.49541497230529785]\n",
            "[Epoch 8/30] [Batch 15/66] [D loss: 4.641844134312123e-05] [G loss: 0.4426305890083313]\n",
            "[Epoch 8/30] [Batch 16/66] [D loss: 4.5650640458916314e-05] [G loss: 0.48622941970825195]\n",
            "[Epoch 8/30] [Batch 17/66] [D loss: 4.4547743527800776e-05] [G loss: 0.4496013820171356]\n",
            "[Epoch 8/30] [Batch 18/66] [D loss: 4.567817086353898e-05] [G loss: 0.480399489402771]\n",
            "[Epoch 8/30] [Batch 19/66] [D loss: 4.5168317228672095e-05] [G loss: 0.43784821033477783]\n",
            "[Epoch 8/30] [Batch 20/66] [D loss: 4.386001091916114e-05] [G loss: 0.45513737201690674]\n",
            "[Epoch 8/30] [Batch 21/66] [D loss: 4.28794082836248e-05] [G loss: 0.47646433115005493]\n",
            "[Epoch 8/30] [Batch 22/66] [D loss: 4.445933336683083e-05] [G loss: 0.44045594334602356]\n",
            "[Epoch 8/30] [Batch 23/66] [D loss: 4.714470014732797e-05] [G loss: 0.5363962650299072]\n",
            "[Epoch 8/30] [Batch 24/66] [D loss: 4.505986908043269e-05] [G loss: 0.4610635042190552]\n",
            "[Epoch 8/30] [Batch 25/66] [D loss: 4.370900205685757e-05] [G loss: 0.6317890882492065]\n",
            "[Epoch 8/30] [Batch 26/66] [D loss: 4.567230644170195e-05] [G loss: 0.527985155582428]\n",
            "[Epoch 8/30] [Batch 27/66] [D loss: 4.333401011535898e-05] [G loss: 0.4365774393081665]\n",
            "[Epoch 8/30] [Batch 28/66] [D loss: 4.30612126365304e-05] [G loss: 0.44158726930618286]\n",
            "[Epoch 8/30] [Batch 29/66] [D loss: 4.3905751226702705e-05] [G loss: 0.4946148097515106]\n",
            "[Epoch 8/30] [Batch 30/66] [D loss: 4.36214995716e-05] [G loss: 0.4669135808944702]\n",
            "[Epoch 8/30] [Batch 31/66] [D loss: 4.536083906714339e-05] [G loss: 0.5509105920791626]\n",
            "[Epoch 8/30] [Batch 32/66] [D loss: 4.648240064852871e-05] [G loss: 0.5341015458106995]\n",
            "[Epoch 8/30] [Batch 33/66] [D loss: 4.300650107325055e-05] [G loss: 0.48915547132492065]\n",
            "[Epoch 8/30] [Batch 34/66] [D loss: 4.419099605001975e-05] [G loss: 0.4261605143547058]\n",
            "[Epoch 8/30] [Batch 35/66] [D loss: 4.278066444385331e-05] [G loss: 0.48742181062698364]\n",
            "[Epoch 8/30] [Batch 36/66] [D loss: 4.261269714334048e-05] [G loss: 0.5113534331321716]\n",
            "[Epoch 8/30] [Batch 37/66] [D loss: 4.472908403840847e-05] [G loss: 0.45523330569267273]\n",
            "[Epoch 8/30] [Batch 38/66] [D loss: 4.172517037659418e-05] [G loss: 0.48763108253479004]\n",
            "[Epoch 8/30] [Batch 39/66] [D loss: 4.124229235458188e-05] [G loss: 0.5501004457473755]\n",
            "[Epoch 8/30] [Batch 40/66] [D loss: 4.156378963671159e-05] [G loss: 0.43805932998657227]\n",
            "[Epoch 8/30] [Batch 41/66] [D loss: 4.3648142309393734e-05] [G loss: 0.5874711275100708]\n",
            "[Epoch 8/30] [Batch 42/66] [D loss: 4.3444608309073374e-05] [G loss: 0.47958266735076904]\n",
            "[Epoch 8/30] [Batch 43/66] [D loss: 4.217631430947222e-05] [G loss: 0.45740848779678345]\n",
            "[Epoch 8/30] [Batch 44/66] [D loss: 4.138463191338815e-05] [G loss: 0.4949578642845154]\n",
            "[Epoch 8/30] [Batch 45/66] [D loss: 4.1720690205693245e-05] [G loss: 0.5852445960044861]\n",
            "[Epoch 8/30] [Batch 46/66] [D loss: 4.158170850132592e-05] [G loss: 0.47679001092910767]\n",
            "[Epoch 8/30] [Batch 47/66] [D loss: 4.102045568288304e-05] [G loss: 0.4725949168205261]\n",
            "[Epoch 8/30] [Batch 48/66] [D loss: 4.098564932064619e-05] [G loss: 0.4874821901321411]\n",
            "[Epoch 8/30] [Batch 49/66] [D loss: 4.274420643923804e-05] [G loss: 0.5049114227294922]\n",
            "[Epoch 8/30] [Batch 50/66] [D loss: 4.2686086089815944e-05] [G loss: 0.5063250064849854]\n",
            "[Epoch 8/30] [Batch 51/66] [D loss: 4.091194932698272e-05] [G loss: 0.5091816782951355]\n",
            "[Epoch 8/30] [Batch 52/66] [D loss: 4.071681360073853e-05] [G loss: 0.4865744709968567]\n",
            "[Epoch 8/30] [Batch 53/66] [D loss: 4.093012466910295e-05] [G loss: 0.5372997522354126]\n",
            "[Epoch 8/30] [Batch 54/66] [D loss: 4.141057070228271e-05] [G loss: 0.5545594692230225]\n",
            "[Epoch 8/30] [Batch 55/66] [D loss: 4.0194405301008373e-05] [G loss: 0.5321003794670105]\n",
            "[Epoch 8/30] [Batch 56/66] [D loss: 4.04959537263494e-05] [G loss: 0.5059681534767151]\n",
            "[Epoch 8/30] [Batch 57/66] [D loss: 3.9330920117208734e-05] [G loss: 0.4299900233745575]\n",
            "[Epoch 8/30] [Batch 58/66] [D loss: 4.0071037801681086e-05] [G loss: 0.48407262563705444]\n",
            "[Epoch 8/30] [Batch 59/66] [D loss: 4.1368768506799825e-05] [G loss: 0.416803240776062]\n",
            "[Epoch 8/30] [Batch 60/66] [D loss: 4.0030326999840327e-05] [G loss: 0.512125551700592]\n",
            "[Epoch 8/30] [Batch 61/66] [D loss: 3.9076503526302986e-05] [G loss: 0.5106212496757507]\n",
            "[Epoch 8/30] [Batch 62/66] [D loss: 4.238337351125665e-05] [G loss: 0.4506942331790924]\n",
            "[Epoch 8/30] [Batch 63/66] [D loss: 4.119888399145566e-05] [G loss: 0.4258089065551758]\n",
            "[Epoch 8/30] [Batch 64/66] [D loss: 3.9020396798150614e-05] [G loss: 0.46023908257484436]\n",
            "[Epoch 9/30] [Batch 0/66] [D loss: 3.8837963074911386e-05] [G loss: 0.5055837631225586]\n",
            "[Epoch 9/30] [Batch 1/66] [D loss: 4.002699279226363e-05] [G loss: 0.45221078395843506]\n",
            "[Epoch 9/30] [Batch 2/66] [D loss: 4.0805172830005176e-05] [G loss: 0.44742855429649353]\n",
            "[Epoch 9/30] [Batch 3/66] [D loss: 3.963435301557183e-05] [G loss: 0.48979052901268005]\n",
            "[Epoch 9/30] [Batch 4/66] [D loss: 4.0282835470861755e-05] [G loss: 0.47315430641174316]\n",
            "[Epoch 9/30] [Batch 5/66] [D loss: 3.840688805212267e-05] [G loss: 0.43503475189208984]\n",
            "[Epoch 9/30] [Batch 6/66] [D loss: 3.9350490624201484e-05] [G loss: 0.47118765115737915]\n",
            "[Epoch 9/30] [Batch 7/66] [D loss: 4.0289010939886793e-05] [G loss: 0.4461957812309265]\n",
            "[Epoch 9/30] [Batch 8/66] [D loss: 3.9153348552645184e-05] [G loss: 0.42749685049057007]\n",
            "[Epoch 9/30] [Batch 9/66] [D loss: 3.7360217902460136e-05] [G loss: 0.4882540702819824]\n",
            "[Epoch 9/30] [Batch 10/66] [D loss: 3.90445511584403e-05] [G loss: 0.4493543207645416]\n",
            "[Epoch 9/30] [Batch 11/66] [D loss: 3.836814721580595e-05] [G loss: 0.4904291033744812]\n",
            "[Epoch 9/30] [Batch 12/66] [D loss: 3.781150371651165e-05] [G loss: 0.447950541973114]\n",
            "[Epoch 9/30] [Batch 13/66] [D loss: 3.743351226148661e-05] [G loss: 0.4368792772293091]\n",
            "[Epoch 9/30] [Batch 14/66] [D loss: 3.75206618627999e-05] [G loss: 0.5057244300842285]\n",
            "[Epoch 9/30] [Batch 15/66] [D loss: 3.720222593983635e-05] [G loss: 0.49288177490234375]\n",
            "[Epoch 9/30] [Batch 16/66] [D loss: 3.8395544834202155e-05] [G loss: 0.4902600646018982]\n",
            "[Epoch 9/30] [Batch 17/66] [D loss: 3.720738459378481e-05] [G loss: 0.42955857515335083]\n",
            "[Epoch 9/30] [Batch 18/66] [D loss: 3.819884113909211e-05] [G loss: 0.5337687134742737]\n",
            "[Epoch 9/30] [Batch 19/66] [D loss: 3.686307536554523e-05] [G loss: 0.4310529828071594]\n",
            "[Epoch 9/30] [Batch 20/66] [D loss: 3.6886138332192786e-05] [G loss: 0.5265262126922607]\n",
            "[Epoch 9/30] [Batch 21/66] [D loss: 3.6236113373888656e-05] [G loss: 0.42329227924346924]\n",
            "[Epoch 9/30] [Batch 22/66] [D loss: 3.5748582376982085e-05] [G loss: 0.46989738941192627]\n",
            "[Epoch 9/30] [Batch 23/66] [D loss: 3.7400641303975135e-05] [G loss: 0.454448401927948]\n",
            "[Epoch 9/30] [Batch 24/66] [D loss: 3.742936132766772e-05] [G loss: 0.45863133668899536]\n",
            "[Epoch 9/30] [Batch 25/66] [D loss: 3.788418871408794e-05] [G loss: 0.4546183943748474]\n",
            "[Epoch 9/30] [Batch 26/66] [D loss: 3.638811540440656e-05] [G loss: 0.5705752372741699]\n",
            "[Epoch 9/30] [Batch 27/66] [D loss: 3.9171816752059385e-05] [G loss: 0.5741137862205505]\n",
            "[Epoch 9/30] [Batch 28/66] [D loss: 3.590615961002186e-05] [G loss: 0.4389897584915161]\n",
            "[Epoch 9/30] [Batch 29/66] [D loss: 3.5336635846761055e-05] [G loss: 0.5259223580360413]\n",
            "[Epoch 9/30] [Batch 30/66] [D loss: 3.724136695382185e-05] [G loss: 0.5024183988571167]\n",
            "[Epoch 9/30] [Batch 31/66] [D loss: 3.505611311993562e-05] [G loss: 0.4158094525337219]\n",
            "[Epoch 9/30] [Batch 32/66] [D loss: 3.61173279088689e-05] [G loss: 0.47030186653137207]\n",
            "[Epoch 9/30] [Batch 33/66] [D loss: 3.468682371021714e-05] [G loss: 0.5317824482917786]\n",
            "[Epoch 9/30] [Batch 34/66] [D loss: 3.5856510294252075e-05] [G loss: 0.4731649160385132]\n",
            "[Epoch 9/30] [Batch 35/66] [D loss: 3.547300548234489e-05] [G loss: 0.4628984332084656]\n",
            "[Epoch 9/30] [Batch 36/66] [D loss: 3.580450174922589e-05] [G loss: 0.48500141501426697]\n",
            "[Epoch 9/30] [Batch 37/66] [D loss: 3.805464075412601e-05] [G loss: 0.4387562870979309]\n",
            "[Epoch 9/30] [Batch 38/66] [D loss: 3.6546332921716385e-05] [G loss: 0.48764467239379883]\n",
            "[Epoch 9/30] [Batch 39/66] [D loss: 3.473883043625392e-05] [G loss: 0.48823171854019165]\n",
            "[Epoch 9/30] [Batch 40/66] [D loss: 3.4070792025886476e-05] [G loss: 0.4850161671638489]\n",
            "[Epoch 9/30] [Batch 41/66] [D loss: 3.385237869224511e-05] [G loss: 0.4201432764530182]\n",
            "[Epoch 9/30] [Batch 42/66] [D loss: 3.526420914568007e-05] [G loss: 0.4442064166069031]\n",
            "[Epoch 9/30] [Batch 43/66] [D loss: 3.639637725427747e-05] [G loss: 0.5048497915267944]\n",
            "[Epoch 9/30] [Batch 44/66] [D loss: 3.3866996091092005e-05] [G loss: 0.47707676887512207]\n",
            "[Epoch 9/30] [Batch 45/66] [D loss: 3.427059891691897e-05] [G loss: 0.4502595067024231]\n",
            "[Epoch 9/30] [Batch 46/66] [D loss: 3.532685150275938e-05] [G loss: 0.45111149549484253]\n",
            "[Epoch 9/30] [Batch 47/66] [D loss: 3.424590795475524e-05] [G loss: 0.42004087567329407]\n",
            "[Epoch 9/30] [Batch 48/66] [D loss: 3.409567034395877e-05] [G loss: 0.45234835147857666]\n",
            "[Epoch 9/30] [Batch 49/66] [D loss: 3.4078162570949644e-05] [G loss: 0.4158591032028198]\n",
            "[Epoch 9/30] [Batch 50/66] [D loss: 3.452145028859377e-05] [G loss: 0.42335328459739685]\n",
            "[Epoch 9/30] [Batch 51/66] [D loss: 3.510438546072692e-05] [G loss: 0.4794389605522156]\n",
            "[Epoch 9/30] [Batch 52/66] [D loss: 3.535690120770596e-05] [G loss: 0.4720667600631714]\n",
            "[Epoch 9/30] [Batch 53/66] [D loss: 3.3951808291021734e-05] [G loss: 0.4787478446960449]\n",
            "[Epoch 9/30] [Batch 54/66] [D loss: 3.363721407367848e-05] [G loss: 0.41450071334838867]\n",
            "[Epoch 9/30] [Batch 55/66] [D loss: 3.482866668491624e-05] [G loss: 0.49039119482040405]\n",
            "[Epoch 9/30] [Batch 56/66] [D loss: 3.577120151021518e-05] [G loss: 0.5418827533721924]\n",
            "[Epoch 9/30] [Batch 57/66] [D loss: 3.466625821602065e-05] [G loss: 0.42525556683540344]\n",
            "[Epoch 9/30] [Batch 58/66] [D loss: 3.3743970561772585e-05] [G loss: 0.4097447693347931]\n",
            "[Epoch 9/30] [Batch 59/66] [D loss: 3.381482565600891e-05] [G loss: 0.4870164394378662]\n",
            "[Epoch 9/30] [Batch 60/66] [D loss: 3.242412822146434e-05] [G loss: 0.41955047845840454]\n",
            "[Epoch 9/30] [Batch 61/66] [D loss: 3.430541073612403e-05] [G loss: 0.42793071269989014]\n",
            "[Epoch 9/30] [Batch 62/66] [D loss: 3.363275027368218e-05] [G loss: 0.4662728011608124]\n",
            "[Epoch 9/30] [Batch 63/66] [D loss: 3.2636660762364045e-05] [G loss: 0.42457568645477295]\n",
            "[Epoch 9/30] [Batch 64/66] [D loss: 3.241842568968423e-05] [G loss: 0.401816725730896]\n",
            "[Epoch 10/30] [Batch 0/66] [D loss: 3.427117371757049e-05] [G loss: 0.5146771669387817]\n",
            "[Epoch 10/30] [Batch 1/66] [D loss: 3.275198832852766e-05] [G loss: 0.47764095664024353]\n",
            "[Epoch 10/30] [Batch 2/66] [D loss: 3.206758810847532e-05] [G loss: 0.3969177007675171]\n",
            "[Epoch 10/30] [Batch 3/66] [D loss: 3.219779500795994e-05] [G loss: 0.46535569429397583]\n",
            "[Epoch 10/30] [Batch 4/66] [D loss: 3.2675524380465504e-05] [G loss: 0.47250795364379883]\n",
            "[Epoch 10/30] [Batch 5/66] [D loss: 3.361727976880502e-05] [G loss: 0.42043039202690125]\n",
            "[Epoch 10/30] [Batch 6/66] [D loss: 3.2506972274859436e-05] [G loss: 0.4363948106765747]\n",
            "[Epoch 10/30] [Batch 7/66] [D loss: 3.31400879076682e-05] [G loss: 0.46405813097953796]\n",
            "[Epoch 10/30] [Batch 8/66] [D loss: 3.283570913481526e-05] [G loss: 0.45395246148109436]\n",
            "[Epoch 10/30] [Batch 9/66] [D loss: 3.1822699384065345e-05] [G loss: 0.501211404800415]\n",
            "[Epoch 10/30] [Batch 10/66] [D loss: 3.128697062493302e-05] [G loss: 0.4052697718143463]\n",
            "[Epoch 10/30] [Batch 11/66] [D loss: 3.2043490136857145e-05] [G loss: 0.4294418692588806]\n",
            "[Epoch 10/30] [Batch 12/66] [D loss: 3.360753908054903e-05] [G loss: 0.42942357063293457]\n",
            "[Epoch 10/30] [Batch 13/66] [D loss: 3.360355731274467e-05] [G loss: 0.4516524076461792]\n",
            "[Epoch 10/30] [Batch 14/66] [D loss: 3.202764128218405e-05] [G loss: 0.44639530777931213]\n",
            "[Epoch 10/30] [Batch 15/66] [D loss: 3.0912275178707205e-05] [G loss: 0.4304254949092865]\n",
            "[Epoch 10/30] [Batch 16/66] [D loss: 3.107928750978317e-05] [G loss: 0.39767950773239136]\n",
            "[Epoch 10/30] [Batch 17/66] [D loss: 3.2091162211145274e-05] [G loss: 0.48626893758773804]\n",
            "[Epoch 10/30] [Batch 18/66] [D loss: 3.128530443063937e-05] [G loss: 0.5628112554550171]\n",
            "[Epoch 10/30] [Batch 19/66] [D loss: 3.279259726696182e-05] [G loss: 0.5042855739593506]\n",
            "[Epoch 10/30] [Batch 20/66] [D loss: 3.1583451345795766e-05] [G loss: 0.4919421076774597]\n",
            "[Epoch 10/30] [Batch 21/66] [D loss: 3.1996674806578085e-05] [G loss: 0.5035241842269897]\n",
            "[Epoch 10/30] [Batch 22/66] [D loss: 3.039568582607899e-05] [G loss: 0.5073624849319458]\n",
            "[Epoch 10/30] [Batch 23/66] [D loss: 3.119815119134728e-05] [G loss: 0.4237666726112366]\n",
            "[Epoch 10/30] [Batch 24/66] [D loss: 3.153712714265566e-05] [G loss: 0.4651414155960083]\n",
            "[Epoch 10/30] [Batch 25/66] [D loss: 3.155137073918013e-05] [G loss: 0.4774830937385559]\n",
            "[Epoch 10/30] [Batch 26/66] [D loss: 3.164037480019033e-05] [G loss: 0.46865880489349365]\n",
            "[Epoch 10/30] [Batch 27/66] [D loss: 3.2244402973446995e-05] [G loss: 0.4702780842781067]\n",
            "[Epoch 10/30] [Batch 28/66] [D loss: 3.0962373784859665e-05] [G loss: 0.4064456820487976]\n",
            "[Epoch 10/30] [Batch 29/66] [D loss: 3.0956725822761655e-05] [G loss: 0.5305113792419434]\n",
            "[Epoch 10/30] [Batch 30/66] [D loss: 3.070655475312378e-05] [G loss: 0.44668832421302795]\n",
            "[Epoch 10/30] [Batch 31/66] [D loss: 3.102666414633859e-05] [G loss: 0.41051578521728516]\n",
            "[Epoch 10/30] [Batch 32/66] [D loss: 3.033475331903901e-05] [G loss: 0.4248344898223877]\n",
            "[Epoch 10/30] [Batch 33/66] [D loss: 3.0581188184442e-05] [G loss: 0.4334709346294403]\n",
            "[Epoch 10/30] [Batch 34/66] [D loss: 3.061157258343883e-05] [G loss: 0.4444733262062073]\n",
            "[Epoch 10/30] [Batch 35/66] [D loss: 2.9130946131772362e-05] [G loss: 0.430794358253479]\n",
            "[Epoch 10/30] [Batch 36/66] [D loss: 3.170742456859443e-05] [G loss: 0.44104835391044617]\n",
            "[Epoch 10/30] [Batch 37/66] [D loss: 3.088105859205825e-05] [G loss: 0.41726016998291016]\n",
            "[Epoch 10/30] [Batch 38/66] [D loss: 3.0502975278068334e-05] [G loss: 0.4102238118648529]\n",
            "[Epoch 10/30] [Batch 39/66] [D loss: 3.0793005862506106e-05] [G loss: 0.4116266965866089]\n",
            "[Epoch 10/30] [Batch 40/66] [D loss: 2.967112322949106e-05] [G loss: 0.618353545665741]\n",
            "[Epoch 10/30] [Batch 41/66] [D loss: 3.152759381919168e-05] [G loss: 0.5077035427093506]\n",
            "[Epoch 10/30] [Batch 42/66] [D loss: 2.8720133741444442e-05] [G loss: 0.5264679789543152]\n",
            "[Epoch 10/30] [Batch 43/66] [D loss: 2.9025348339928314e-05] [G loss: 0.5025395154953003]\n",
            "[Epoch 10/30] [Batch 44/66] [D loss: 2.9517685106839053e-05] [G loss: 0.4734126329421997]\n",
            "[Epoch 10/30] [Batch 45/66] [D loss: 2.8419990485417657e-05] [G loss: 0.4421057105064392]\n",
            "[Epoch 10/30] [Batch 46/66] [D loss: 2.9187611289671622e-05] [G loss: 0.4665897488594055]\n",
            "[Epoch 10/30] [Batch 47/66] [D loss: 2.9250775696709752e-05] [G loss: 0.4164981544017792]\n",
            "[Epoch 10/30] [Batch 48/66] [D loss: 2.9605936106236186e-05] [G loss: 0.39072513580322266]\n",
            "[Epoch 10/30] [Batch 49/66] [D loss: 2.900550862250384e-05] [G loss: 0.39542967081069946]\n",
            "[Epoch 10/30] [Batch 50/66] [D loss: 2.9382355023699347e-05] [G loss: 0.43166327476501465]\n",
            "[Epoch 10/30] [Batch 51/66] [D loss: 2.839803164533805e-05] [G loss: 0.49463194608688354]\n",
            "[Epoch 10/30] [Batch 52/66] [D loss: 2.8540792300191242e-05] [G loss: 0.46577417850494385]\n",
            "[Epoch 10/30] [Batch 53/66] [D loss: 2.8767934054485522e-05] [G loss: 0.5164005756378174]\n",
            "[Epoch 10/30] [Batch 54/66] [D loss: 2.8611374546017032e-05] [G loss: 0.41026222705841064]\n",
            "[Epoch 10/30] [Batch 55/66] [D loss: 2.934363692475017e-05] [G loss: 0.4886898994445801]\n",
            "[Epoch 10/30] [Batch 56/66] [D loss: 2.9136334887880366e-05] [G loss: 0.42352771759033203]\n",
            "[Epoch 10/30] [Batch 57/66] [D loss: 2.8950319574505556e-05] [G loss: 0.38874566555023193]\n",
            "[Epoch 10/30] [Batch 58/66] [D loss: 2.8637955438171048e-05] [G loss: 0.4364381432533264]\n",
            "[Epoch 10/30] [Batch 59/66] [D loss: 2.8548822228913195e-05] [G loss: 0.4350195527076721]\n",
            "[Epoch 10/30] [Batch 60/66] [D loss: 2.8026594009133987e-05] [G loss: 0.4706103205680847]\n",
            "[Epoch 10/30] [Batch 61/66] [D loss: 2.9299169000296388e-05] [G loss: 0.4648957848548889]\n",
            "[Epoch 10/30] [Batch 62/66] [D loss: 2.823574232024839e-05] [G loss: 0.4174545407295227]\n",
            "[Epoch 10/30] [Batch 63/66] [D loss: 2.8651163120230194e-05] [G loss: 0.4023176431655884]\n",
            "[Epoch 10/30] [Batch 64/66] [D loss: 2.9377620194281917e-05] [G loss: 0.44910818338394165]\n",
            "[Epoch 11/30] [Batch 0/66] [D loss: 2.7930618671234697e-05] [G loss: 0.4064342975616455]\n",
            "[Epoch 11/30] [Batch 1/66] [D loss: 2.7583548217080534e-05] [G loss: 0.39726048707962036]\n",
            "[Epoch 11/30] [Batch 2/66] [D loss: 2.8754089726135135e-05] [G loss: 0.42253994941711426]\n",
            "[Epoch 11/30] [Batch 3/66] [D loss: 2.8105335331929382e-05] [G loss: 0.49754172563552856]\n",
            "[Epoch 11/30] [Batch 4/66] [D loss: 2.911284173023887e-05] [G loss: 0.44785043597221375]\n",
            "[Epoch 11/30] [Batch 5/66] [D loss: 2.7818387025035918e-05] [G loss: 0.40963149070739746]\n",
            "[Epoch 11/30] [Batch 6/66] [D loss: 2.7178319214726798e-05] [G loss: 0.4063915014266968]\n",
            "[Epoch 11/30] [Batch 7/66] [D loss: 2.7362162654753774e-05] [G loss: 0.39645808935165405]\n",
            "[Epoch 11/30] [Batch 8/66] [D loss: 2.791042425087653e-05] [G loss: 0.42279499769210815]\n",
            "[Epoch 11/30] [Batch 9/66] [D loss: 2.8023549930367153e-05] [G loss: 0.4581384062767029]\n",
            "[Epoch 11/30] [Batch 10/66] [D loss: 2.7340283850207925e-05] [G loss: 0.46130239963531494]\n",
            "[Epoch 11/30] [Batch 11/66] [D loss: 2.9499755328288302e-05] [G loss: 0.48603957891464233]\n",
            "[Epoch 11/30] [Batch 12/66] [D loss: 2.8049636057403404e-05] [G loss: 0.45816636085510254]\n",
            "[Epoch 11/30] [Batch 13/66] [D loss: 2.6676542802306358e-05] [G loss: 0.4358505606651306]\n",
            "[Epoch 11/30] [Batch 14/66] [D loss: 2.7768672225647606e-05] [G loss: 0.43245089054107666]\n",
            "[Epoch 11/30] [Batch 15/66] [D loss: 2.834504448401276e-05] [G loss: 0.43059486150741577]\n",
            "[Epoch 11/30] [Batch 16/66] [D loss: 2.748289807641413e-05] [G loss: 0.4489343762397766]\n",
            "[Epoch 11/30] [Batch 17/66] [D loss: 2.659658457559999e-05] [G loss: 0.4628671407699585]\n",
            "[Epoch 11/30] [Batch 18/66] [D loss: 2.8078055038349703e-05] [G loss: 0.42258983850479126]\n",
            "[Epoch 11/30] [Batch 19/66] [D loss: 2.716674225666793e-05] [G loss: 0.4450492858886719]\n",
            "[Epoch 11/30] [Batch 20/66] [D loss: 2.714452421059832e-05] [G loss: 0.5132993459701538]\n",
            "[Epoch 11/30] [Batch 21/66] [D loss: 2.6864947358262725e-05] [G loss: 0.390366792678833]\n",
            "[Epoch 11/30] [Batch 22/66] [D loss: 2.664423118403647e-05] [G loss: 0.5356665849685669]\n",
            "[Epoch 11/30] [Batch 23/66] [D loss: 2.7522328309714794e-05] [G loss: 0.39966535568237305]\n",
            "[Epoch 11/30] [Batch 24/66] [D loss: 2.6683723262976855e-05] [G loss: 0.45477503538131714]\n",
            "[Epoch 11/30] [Batch 25/66] [D loss: 2.7288218916510232e-05] [G loss: 0.49807316064834595]\n",
            "[Epoch 11/30] [Batch 26/66] [D loss: 2.707206931518158e-05] [G loss: 0.4435842037200928]\n",
            "[Epoch 11/30] [Batch 27/66] [D loss: 2.688910626602592e-05] [G loss: 0.4374203383922577]\n",
            "[Epoch 11/30] [Batch 28/66] [D loss: 2.6048955078294966e-05] [G loss: 0.42544955015182495]\n",
            "[Epoch 11/30] [Batch 29/66] [D loss: 2.6510713723837398e-05] [G loss: 0.4337989091873169]\n",
            "[Epoch 11/30] [Batch 30/66] [D loss: 2.6810945200850256e-05] [G loss: 0.44610288739204407]\n",
            "[Epoch 11/30] [Batch 31/66] [D loss: 2.5942925276467577e-05] [G loss: 0.4535602033138275]\n",
            "[Epoch 11/30] [Batch 32/66] [D loss: 2.65064054474351e-05] [G loss: 0.41086262464523315]\n",
            "[Epoch 11/30] [Batch 33/66] [D loss: 2.5556366381351836e-05] [G loss: 0.40688401460647583]\n",
            "[Epoch 11/30] [Batch 34/66] [D loss: 2.7478544325276744e-05] [G loss: 0.45419803261756897]\n",
            "[Epoch 11/30] [Batch 35/66] [D loss: 2.7094817596662324e-05] [G loss: 0.45642316341400146]\n",
            "[Epoch 11/30] [Batch 36/66] [D loss: 2.6979383619618602e-05] [G loss: 0.4924112558364868]\n",
            "[Epoch 11/30] [Batch 37/66] [D loss: 2.5233557607862167e-05] [G loss: 0.38357067108154297]\n",
            "[Epoch 11/30] [Batch 38/66] [D loss: 2.5457999072386883e-05] [G loss: 0.4121088683605194]\n",
            "[Epoch 11/30] [Batch 39/66] [D loss: 2.6371179956186097e-05] [G loss: 0.4324009418487549]\n",
            "[Epoch 11/30] [Batch 40/66] [D loss: 2.570555625425186e-05] [G loss: 0.460948646068573]\n",
            "[Epoch 11/30] [Batch 41/66] [D loss: 2.6215328034595586e-05] [G loss: 0.4121452569961548]\n",
            "[Epoch 11/30] [Batch 42/66] [D loss: 2.6470632292330265e-05] [G loss: 0.47589635848999023]\n",
            "[Epoch 11/30] [Batch 43/66] [D loss: 2.5627085051382892e-05] [G loss: 0.44549113512039185]\n",
            "[Epoch 11/30] [Batch 44/66] [D loss: 2.5847232791420538e-05] [G loss: 0.5140985250473022]\n",
            "[Epoch 11/30] [Batch 45/66] [D loss: 2.580197724455502e-05] [G loss: 0.4492480754852295]\n",
            "[Epoch 11/30] [Batch 46/66] [D loss: 2.4358429982385132e-05] [G loss: 0.4377620220184326]\n",
            "[Epoch 11/30] [Batch 47/66] [D loss: 2.6026445084426086e-05] [G loss: 0.4405035078525543]\n",
            "[Epoch 11/30] [Batch 48/66] [D loss: 2.5552581064403057e-05] [G loss: 0.4535094201564789]\n",
            "[Epoch 11/30] [Batch 49/66] [D loss: 2.5616289349272847e-05] [G loss: 0.4277470111846924]\n",
            "[Epoch 11/30] [Batch 50/66] [D loss: 2.516071981517598e-05] [G loss: 0.42112892866134644]\n",
            "[Epoch 11/30] [Batch 51/66] [D loss: 2.5499160983599722e-05] [G loss: 0.39494454860687256]\n",
            "[Epoch 11/30] [Batch 52/66] [D loss: 2.5345463654957712e-05] [G loss: 0.4666770398616791]\n",
            "[Epoch 11/30] [Batch 53/66] [D loss: 2.4672172003192827e-05] [G loss: 0.4378882646560669]\n",
            "[Epoch 11/30] [Batch 54/66] [D loss: 2.508774650777923e-05] [G loss: 0.4117315113544464]\n",
            "[Epoch 11/30] [Batch 55/66] [D loss: 2.5312624529760797e-05] [G loss: 0.39008164405822754]\n",
            "[Epoch 11/30] [Batch 56/66] [D loss: 2.463272903696634e-05] [G loss: 0.4727552533149719]\n",
            "[Epoch 11/30] [Batch 57/66] [D loss: 2.5883231501211412e-05] [G loss: 0.43801605701446533]\n",
            "[Epoch 11/30] [Batch 58/66] [D loss: 2.410157412668923e-05] [G loss: 0.36629217863082886]\n",
            "[Epoch 11/30] [Batch 59/66] [D loss: 2.438238971080864e-05] [G loss: 0.43203848600387573]\n",
            "[Epoch 11/30] [Batch 60/66] [D loss: 2.5089409973588772e-05] [G loss: 0.4318224787712097]\n",
            "[Epoch 11/30] [Batch 61/66] [D loss: 2.4199390281864908e-05] [G loss: 0.392472505569458]\n",
            "[Epoch 11/30] [Batch 62/66] [D loss: 2.4218769794970285e-05] [G loss: 0.435829222202301]\n",
            "[Epoch 11/30] [Batch 63/66] [D loss: 2.4105751435854472e-05] [G loss: 0.4534120559692383]\n",
            "[Epoch 11/30] [Batch 64/66] [D loss: 2.4821198167046532e-05] [G loss: 0.387146532535553]\n",
            "[Epoch 12/30] [Batch 0/66] [D loss: 2.4656754249008372e-05] [G loss: 0.41522282361984253]\n",
            "[Epoch 12/30] [Batch 1/66] [D loss: 2.432189376122551e-05] [G loss: 0.5718369483947754]\n",
            "[Epoch 12/30] [Batch 2/66] [D loss: 2.4051576474448666e-05] [G loss: 0.45147091150283813]\n",
            "[Epoch 12/30] [Batch 3/66] [D loss: 2.372670769545948e-05] [G loss: 0.4454743564128876]\n",
            "[Epoch 12/30] [Batch 4/66] [D loss: 2.474278608133318e-05] [G loss: 0.42524057626724243]\n",
            "[Epoch 12/30] [Batch 5/66] [D loss: 2.4603276870038826e-05] [G loss: 0.4714357852935791]\n",
            "[Epoch 12/30] [Batch 6/66] [D loss: 2.448470149829518e-05] [G loss: 0.3940216302871704]\n",
            "[Epoch 12/30] [Batch 7/66] [D loss: 2.3754944777465425e-05] [G loss: 0.4725687503814697]\n",
            "[Epoch 12/30] [Batch 8/66] [D loss: 2.3743833480693866e-05] [G loss: 0.38795679807662964]\n",
            "[Epoch 12/30] [Batch 9/66] [D loss: 2.407159536232939e-05] [G loss: 0.39106765389442444]\n",
            "[Epoch 12/30] [Batch 10/66] [D loss: 2.404663791821804e-05] [G loss: 0.44579899311065674]\n",
            "[Epoch 12/30] [Batch 11/66] [D loss: 2.4817820303724147e-05] [G loss: 0.4182935357093811]\n",
            "[Epoch 12/30] [Batch 12/66] [D loss: 2.450720057822764e-05] [G loss: 0.43114402890205383]\n",
            "[Epoch 12/30] [Batch 13/66] [D loss: 2.446723829052644e-05] [G loss: 0.4787282943725586]\n",
            "[Epoch 12/30] [Batch 14/66] [D loss: 2.2812561837781686e-05] [G loss: 0.408633828163147]\n",
            "[Epoch 12/30] [Batch 15/66] [D loss: 2.300019968970446e-05] [G loss: 0.4620298743247986]\n",
            "[Epoch 12/30] [Batch 16/66] [D loss: 2.2839554731035605e-05] [G loss: 0.44366875290870667]\n",
            "[Epoch 12/30] [Batch 17/66] [D loss: 2.3548718672827817e-05] [G loss: 0.4285620450973511]\n",
            "[Epoch 12/30] [Batch 18/66] [D loss: 2.3943762244016398e-05] [G loss: 0.4696866571903229]\n",
            "[Epoch 12/30] [Batch 19/66] [D loss: 2.4007623323996086e-05] [G loss: 0.3941948413848877]\n",
            "[Epoch 12/30] [Batch 20/66] [D loss: 2.393819704593625e-05] [G loss: 0.4505987763404846]\n",
            "[Epoch 12/30] [Batch 21/66] [D loss: 2.3698878976574633e-05] [G loss: 0.3824300169944763]\n",
            "[Epoch 12/30] [Batch 22/66] [D loss: 2.375248914177064e-05] [G loss: 0.42503759264945984]\n",
            "[Epoch 12/30] [Batch 23/66] [D loss: 2.340960236324463e-05] [G loss: 0.44038307666778564]\n",
            "[Epoch 12/30] [Batch 24/66] [D loss: 2.3635464458493516e-05] [G loss: 0.4594630002975464]\n",
            "[Epoch 12/30] [Batch 25/66] [D loss: 2.3480813979404047e-05] [G loss: 0.4586494565010071]\n",
            "[Epoch 12/30] [Batch 26/66] [D loss: 2.397972912149271e-05] [G loss: 0.46227264404296875]\n",
            "[Epoch 12/30] [Batch 27/66] [D loss: 2.433861664030701e-05] [G loss: 0.4532009959220886]\n",
            "[Epoch 12/30] [Batch 28/66] [D loss: 2.280763874296099e-05] [G loss: 0.4334760904312134]\n",
            "[Epoch 12/30] [Batch 29/66] [D loss: 2.278447209391743e-05] [G loss: 0.41155558824539185]\n",
            "[Epoch 12/30] [Batch 30/66] [D loss: 2.295372905791737e-05] [G loss: 0.5510780811309814]\n",
            "[Epoch 12/30] [Batch 31/66] [D loss: 2.3098040401237085e-05] [G loss: 0.48635953664779663]\n",
            "[Epoch 12/30] [Batch 32/66] [D loss: 2.3618784325663e-05] [G loss: 0.40323734283447266]\n",
            "[Epoch 12/30] [Batch 33/66] [D loss: 2.399037839495577e-05] [G loss: 0.4709111452102661]\n",
            "[Epoch 12/30] [Batch 34/66] [D loss: 2.2975008505454753e-05] [G loss: 0.4051101505756378]\n",
            "[Epoch 12/30] [Batch 35/66] [D loss: 2.2711856217938475e-05] [G loss: 0.4072571396827698]\n",
            "[Epoch 12/30] [Batch 36/66] [D loss: 2.1800564354634844e-05] [G loss: 0.3752692639827728]\n",
            "[Epoch 12/30] [Batch 37/66] [D loss: 2.3346892703557387e-05] [G loss: 0.5264900922775269]\n",
            "[Epoch 12/30] [Batch 38/66] [D loss: 2.2503400941786822e-05] [G loss: 0.4709499478340149]\n",
            "[Epoch 12/30] [Batch 39/66] [D loss: 2.2818949219072238e-05] [G loss: 0.39150357246398926]\n",
            "[Epoch 12/30] [Batch 40/66] [D loss: 2.366876469750423e-05] [G loss: 0.4883095622062683]\n",
            "[Epoch 12/30] [Batch 41/66] [D loss: 2.196710011048708e-05] [G loss: 0.44099608063697815]\n",
            "[Epoch 12/30] [Batch 42/66] [D loss: 2.3200427676783875e-05] [G loss: 0.36442434787750244]\n",
            "[Epoch 12/30] [Batch 43/66] [D loss: 2.2385580450645648e-05] [G loss: 0.5094606280326843]\n",
            "[Epoch 12/30] [Batch 44/66] [D loss: 2.225898151664296e-05] [G loss: 0.4418257474899292]\n",
            "[Epoch 12/30] [Batch 45/66] [D loss: 2.1494002794497646e-05] [G loss: 0.5623824000358582]\n",
            "[Epoch 12/30] [Batch 46/66] [D loss: 2.275084625580348e-05] [G loss: 0.48978391289711]\n",
            "[Epoch 12/30] [Batch 47/66] [D loss: 2.23410288526793e-05] [G loss: 0.42801111936569214]\n",
            "[Epoch 12/30] [Batch 48/66] [D loss: 2.3276365027413704e-05] [G loss: 0.44651734828948975]\n",
            "[Epoch 12/30] [Batch 49/66] [D loss: 2.2325593818095513e-05] [G loss: 0.4330088794231415]\n",
            "[Epoch 12/30] [Batch 50/66] [D loss: 2.1726975319324993e-05] [G loss: 0.42387399077415466]\n",
            "[Epoch 12/30] [Batch 51/66] [D loss: 2.154721732949838e-05] [G loss: 0.46799033880233765]\n",
            "[Epoch 12/30] [Batch 52/66] [D loss: 2.2172134777065367e-05] [G loss: 0.3982178568840027]\n",
            "[Epoch 12/30] [Batch 53/66] [D loss: 2.3099270038073882e-05] [G loss: 0.509706974029541]\n",
            "[Epoch 12/30] [Batch 54/66] [D loss: 2.186031542805722e-05] [G loss: 0.3985939621925354]\n",
            "[Epoch 12/30] [Batch 55/66] [D loss: 2.1687835214834195e-05] [G loss: 0.39229902625083923]\n",
            "[Epoch 12/30] [Batch 56/66] [D loss: 2.274791677336907e-05] [G loss: 0.41347795724868774]\n",
            "[Epoch 12/30] [Batch 57/66] [D loss: 2.203623535024235e-05] [G loss: 0.41713979840278625]\n",
            "[Epoch 12/30] [Batch 58/66] [D loss: 2.2003406229487155e-05] [G loss: 0.38765862584114075]\n",
            "[Epoch 12/30] [Batch 59/66] [D loss: 2.050528291874798e-05] [G loss: 0.5305975079536438]\n",
            "[Epoch 12/30] [Batch 60/66] [D loss: 2.1960540834697895e-05] [G loss: 0.46501749753952026]\n",
            "[Epoch 12/30] [Batch 61/66] [D loss: 2.1590640244539827e-05] [G loss: 0.40097272396087646]\n",
            "[Epoch 12/30] [Batch 62/66] [D loss: 2.2534680283570196e-05] [G loss: 0.550761342048645]\n",
            "[Epoch 12/30] [Batch 63/66] [D loss: 2.139489515684545e-05] [G loss: 0.40959662199020386]\n",
            "[Epoch 12/30] [Batch 64/66] [D loss: 2.0682065951405093e-05] [G loss: 0.45082640647888184]\n",
            "[Epoch 13/30] [Batch 0/66] [D loss: 2.1570109311142005e-05] [G loss: 0.3890250325202942]\n",
            "[Epoch 13/30] [Batch 1/66] [D loss: 2.218879308202304e-05] [G loss: 0.39724549651145935]\n",
            "[Epoch 13/30] [Batch 2/66] [D loss: 2.182036405429244e-05] [G loss: 0.43804818391799927]\n",
            "[Epoch 13/30] [Batch 3/66] [D loss: 2.090268571919296e-05] [G loss: 0.37045419216156006]\n",
            "[Epoch 13/30] [Batch 4/66] [D loss: 2.1083646061015315e-05] [G loss: 0.3888363242149353]\n",
            "[Epoch 13/30] [Batch 5/66] [D loss: 2.272644269396551e-05] [G loss: 0.4876827597618103]\n",
            "[Epoch 13/30] [Batch 6/66] [D loss: 2.2502069441543426e-05] [G loss: 0.4937094449996948]\n",
            "[Epoch 13/30] [Batch 7/66] [D loss: 2.1012247088947333e-05] [G loss: 0.3894496560096741]\n",
            "[Epoch 13/30] [Batch 8/66] [D loss: 2.0494436284934636e-05] [G loss: 0.4147469401359558]\n",
            "[Epoch 13/30] [Batch 9/66] [D loss: 2.1068609385110904e-05] [G loss: 0.43769919872283936]\n",
            "[Epoch 13/30] [Batch 10/66] [D loss: 2.100628444168251e-05] [G loss: 0.3868071138858795]\n",
            "[Epoch 13/30] [Batch 11/66] [D loss: 2.1757863578386605e-05] [G loss: 0.3860006630420685]\n",
            "[Epoch 13/30] [Batch 12/66] [D loss: 2.0744309040310327e-05] [G loss: 0.38512128591537476]\n",
            "[Epoch 13/30] [Batch 13/66] [D loss: 2.113240225298796e-05] [G loss: 0.43536239862442017]\n",
            "[Epoch 13/30] [Batch 14/66] [D loss: 2.0623790987883694e-05] [G loss: 0.44605177640914917]\n",
            "[Epoch 13/30] [Batch 15/66] [D loss: 2.032434167631436e-05] [G loss: 0.38612526655197144]\n",
            "[Epoch 13/30] [Batch 16/66] [D loss: 2.0547482563415542e-05] [G loss: 0.4077489376068115]\n",
            "[Epoch 13/30] [Batch 17/66] [D loss: 2.03158970180084e-05] [G loss: 0.42302200198173523]\n",
            "[Epoch 13/30] [Batch 18/66] [D loss: 2.1466417820192873e-05] [G loss: 0.4131873846054077]\n",
            "[Epoch 13/30] [Batch 19/66] [D loss: 2.1491976440302096e-05] [G loss: 0.45559969544410706]\n",
            "[Epoch 13/30] [Batch 20/66] [D loss: 2.0678538930951618e-05] [G loss: 0.3779049515724182]\n",
            "[Epoch 13/30] [Batch 21/66] [D loss: 2.1270887373248115e-05] [G loss: 0.4072791635990143]\n",
            "[Epoch 13/30] [Batch 22/66] [D loss: 2.0915406821586657e-05] [G loss: 0.4399101734161377]\n",
            "[Epoch 13/30] [Batch 23/66] [D loss: 2.0332244275778066e-05] [G loss: 0.39361119270324707]\n",
            "[Epoch 13/30] [Batch 24/66] [D loss: 2.011856577155413e-05] [G loss: 0.4289933741092682]\n",
            "[Epoch 13/30] [Batch 25/66] [D loss: 2.0347606550785713e-05] [G loss: 0.41420847177505493]\n",
            "[Epoch 13/30] [Batch 26/66] [D loss: 2.070287700917106e-05] [G loss: 0.4125445485115051]\n",
            "[Epoch 13/30] [Batch 27/66] [D loss: 2.132370718754828e-05] [G loss: 0.3679174482822418]\n",
            "[Epoch 13/30] [Batch 28/66] [D loss: 2.0066678189323284e-05] [G loss: 0.35892608761787415]\n",
            "[Epoch 13/30] [Batch 29/66] [D loss: 2.0613720153050963e-05] [G loss: 0.422621488571167]\n",
            "[Epoch 13/30] [Batch 30/66] [D loss: 1.9796246306214016e-05] [G loss: 0.37866008281707764]\n",
            "[Epoch 13/30] [Batch 31/66] [D loss: 2.0204642169119325e-05] [G loss: 0.49341338872909546]\n",
            "[Epoch 13/30] [Batch 32/66] [D loss: 2.0652624698414e-05] [G loss: 0.43524354696273804]\n",
            "[Epoch 13/30] [Batch 33/66] [D loss: 2.0039511582581326e-05] [G loss: 0.3999851644039154]\n",
            "[Epoch 13/30] [Batch 34/66] [D loss: 1.999852065637242e-05] [G loss: 0.4418659210205078]\n",
            "[Epoch 13/30] [Batch 35/66] [D loss: 1.9111081201117486e-05] [G loss: 0.38422778248786926]\n",
            "[Epoch 13/30] [Batch 36/66] [D loss: 2.0010574189655017e-05] [G loss: 0.42509889602661133]\n",
            "[Epoch 13/30] [Batch 37/66] [D loss: 2.1020947315264493e-05] [G loss: 0.5101203322410583]\n",
            "[Epoch 13/30] [Batch 38/66] [D loss: 1.9600155610532966e-05] [G loss: 0.44773170351982117]\n",
            "[Epoch 13/30] [Batch 39/66] [D loss: 1.9309047274873592e-05] [G loss: 0.45069438219070435]\n",
            "[Epoch 13/30] [Batch 40/66] [D loss: 1.9741731193789747e-05] [G loss: 0.41368669271469116]\n",
            "[Epoch 13/30] [Batch 41/66] [D loss: 1.967759817489423e-05] [G loss: 0.37056466937065125]\n",
            "[Epoch 13/30] [Batch 42/66] [D loss: 1.9678329408634454e-05] [G loss: 0.37052714824676514]\n",
            "[Epoch 13/30] [Batch 43/66] [D loss: 2.062717430817429e-05] [G loss: 0.3728034198284149]\n",
            "[Epoch 13/30] [Batch 44/66] [D loss: 1.9786108168773353e-05] [G loss: 0.4511934518814087]\n",
            "[Epoch 13/30] [Batch 45/66] [D loss: 1.976732073671883e-05] [G loss: 0.36213257908821106]\n",
            "[Epoch 13/30] [Batch 46/66] [D loss: 2.034756380453473e-05] [G loss: 0.4050523340702057]\n",
            "[Epoch 13/30] [Batch 47/66] [D loss: 1.952265029103728e-05] [G loss: 0.42036619782447815]\n",
            "[Epoch 13/30] [Batch 48/66] [D loss: 2.0225064872647636e-05] [G loss: 0.3842863440513611]\n",
            "[Epoch 13/30] [Batch 49/66] [D loss: 2.0155230231466703e-05] [G loss: 0.37357228994369507]\n",
            "[Epoch 13/30] [Batch 50/66] [D loss: 2.0176411453576293e-05] [G loss: 0.3714256286621094]\n",
            "[Epoch 13/30] [Batch 51/66] [D loss: 1.989174234040547e-05] [G loss: 0.4416491389274597]\n",
            "[Epoch 13/30] [Batch 52/66] [D loss: 1.9328081179992296e-05] [G loss: 0.4231360852718353]\n",
            "[Epoch 13/30] [Batch 53/66] [D loss: 1.963940667337738e-05] [G loss: 0.4640422761440277]\n",
            "[Epoch 13/30] [Batch 54/66] [D loss: 1.995969705603784e-05] [G loss: 0.47444212436676025]\n",
            "[Epoch 13/30] [Batch 55/66] [D loss: 1.895349305414129e-05] [G loss: 0.4297979772090912]\n",
            "[Epoch 13/30] [Batch 56/66] [D loss: 1.9244293071096763e-05] [G loss: 0.4119219183921814]\n",
            "[Epoch 13/30] [Batch 57/66] [D loss: 1.957555468834471e-05] [G loss: 0.39184606075286865]\n",
            "[Epoch 13/30] [Batch 58/66] [D loss: 1.9702162717294414e-05] [G loss: 0.4236946105957031]\n",
            "[Epoch 13/30] [Batch 59/66] [D loss: 1.9138607967761345e-05] [G loss: 0.3770894408226013]\n",
            "[Epoch 13/30] [Batch 60/66] [D loss: 1.9349919966771267e-05] [G loss: 0.4191151261329651]\n",
            "[Epoch 13/30] [Batch 61/66] [D loss: 1.9087350665358827e-05] [G loss: 0.39333051443099976]\n",
            "[Epoch 13/30] [Batch 62/66] [D loss: 1.9514954146870878e-05] [G loss: 0.37443649768829346]\n",
            "[Epoch 13/30] [Batch 63/66] [D loss: 1.9208497178624384e-05] [G loss: 0.4146043062210083]\n",
            "[Epoch 13/30] [Batch 64/66] [D loss: 1.946384691109415e-05] [G loss: 0.40190520882606506]\n",
            "[Epoch 14/30] [Batch 0/66] [D loss: 1.9611872630775906e-05] [G loss: 0.38379010558128357]\n",
            "[Epoch 14/30] [Batch 1/66] [D loss: 1.88322264875751e-05] [G loss: 0.3702108860015869]\n",
            "[Epoch 14/30] [Batch 2/66] [D loss: 1.9202314433641732e-05] [G loss: 0.4133695363998413]\n",
            "[Epoch 14/30] [Batch 3/66] [D loss: 1.8860268028220162e-05] [G loss: 0.4179883599281311]\n",
            "[Epoch 14/30] [Batch 4/66] [D loss: 1.8932000784843694e-05] [G loss: 0.3839513063430786]\n",
            "[Epoch 14/30] [Batch 5/66] [D loss: 1.8536885363573674e-05] [G loss: 0.3769248425960541]\n",
            "[Epoch 14/30] [Batch 6/66] [D loss: 1.90883110917639e-05] [G loss: 0.3843940198421478]\n",
            "[Epoch 14/30] [Batch 7/66] [D loss: 1.838589014369063e-05] [G loss: 0.4559478163719177]\n",
            "[Epoch 14/30] [Batch 8/66] [D loss: 1.8752127289189957e-05] [G loss: 0.3766123950481415]\n",
            "[Epoch 14/30] [Batch 9/66] [D loss: 1.8993811863765586e-05] [G loss: 0.3957476317882538]\n",
            "[Epoch 14/30] [Batch 10/66] [D loss: 1.820364013838116e-05] [G loss: 0.42038339376449585]\n",
            "[Epoch 14/30] [Batch 11/66] [D loss: 1.8406733943265863e-05] [G loss: 0.3963956832885742]\n",
            "[Epoch 14/30] [Batch 12/66] [D loss: 1.9130480723106302e-05] [G loss: 0.4147327244281769]\n",
            "[Epoch 14/30] [Batch 13/66] [D loss: 1.8919704416475724e-05] [G loss: 0.42347973585128784]\n",
            "[Epoch 14/30] [Batch 14/66] [D loss: 1.895504192361841e-05] [G loss: 0.41368967294692993]\n",
            "[Epoch 14/30] [Batch 15/66] [D loss: 1.8516473573981784e-05] [G loss: 0.40496838092803955]\n",
            "[Epoch 14/30] [Batch 16/66] [D loss: 1.9644047824840527e-05] [G loss: 0.522887110710144]\n",
            "[Epoch 14/30] [Batch 17/66] [D loss: 1.9487569261400495e-05] [G loss: 0.5546221733093262]\n",
            "[Epoch 14/30] [Batch 18/66] [D loss: 1.827535106713185e-05] [G loss: 0.4810277819633484]\n",
            "[Epoch 14/30] [Batch 19/66] [D loss: 1.935636919370154e-05] [G loss: 0.3785526156425476]\n",
            "[Epoch 14/30] [Batch 20/66] [D loss: 1.8132177501684055e-05] [G loss: 0.44211870431900024]\n",
            "[Epoch 14/30] [Batch 21/66] [D loss: 1.8081201233144384e-05] [G loss: 0.42566221952438354]\n",
            "[Epoch 14/30] [Batch 22/66] [D loss: 1.9004210116690956e-05] [G loss: 0.36798524856567383]\n",
            "[Epoch 14/30] [Batch 23/66] [D loss: 1.8955273844767362e-05] [G loss: 0.5379348993301392]\n",
            "[Epoch 14/30] [Batch 24/66] [D loss: 1.7890702110889833e-05] [G loss: 0.393987238407135]\n",
            "[Epoch 14/30] [Batch 25/66] [D loss: 1.7724535609886516e-05] [G loss: 0.48945605754852295]\n",
            "[Epoch 14/30] [Batch 26/66] [D loss: 1.829685061238706e-05] [G loss: 0.40871939063072205]\n",
            "[Epoch 14/30] [Batch 27/66] [D loss: 1.803730265237391e-05] [G loss: 0.3884879946708679]\n",
            "[Epoch 14/30] [Batch 28/66] [D loss: 1.7898981241160072e-05] [G loss: 0.4588245749473572]\n",
            "[Epoch 14/30] [Batch 29/66] [D loss: 1.7945269064512104e-05] [G loss: 0.44780099391937256]\n",
            "[Epoch 14/30] [Batch 30/66] [D loss: 1.827779215091141e-05] [G loss: 0.37295806407928467]\n",
            "[Epoch 14/30] [Batch 31/66] [D loss: 1.8236598407384008e-05] [G loss: 0.4054175615310669]\n",
            "[Epoch 14/30] [Batch 32/66] [D loss: 1.7756854504114017e-05] [G loss: 0.5057292580604553]\n",
            "[Epoch 14/30] [Batch 33/66] [D loss: 1.8069385987473652e-05] [G loss: 0.40021806955337524]\n",
            "[Epoch 14/30] [Batch 34/66] [D loss: 1.8054776774079073e-05] [G loss: 0.37593990564346313]\n",
            "[Epoch 14/30] [Batch 35/66] [D loss: 1.7986284547077958e-05] [G loss: 0.36674007773399353]\n",
            "[Epoch 14/30] [Batch 36/66] [D loss: 1.7733512322593015e-05] [G loss: 0.40007758140563965]\n",
            "[Epoch 14/30] [Batch 37/66] [D loss: 1.8237976291857194e-05] [G loss: 0.4305373728275299]\n",
            "[Epoch 14/30] [Batch 38/66] [D loss: 1.8106477000401355e-05] [G loss: 0.4529726505279541]\n",
            "[Epoch 14/30] [Batch 39/66] [D loss: 1.8115678358299192e-05] [G loss: 0.3701576292514801]\n",
            "[Epoch 14/30] [Batch 40/66] [D loss: 1.796754168026382e-05] [G loss: 0.36618226766586304]\n",
            "[Epoch 14/30] [Batch 41/66] [D loss: 1.7704986021271907e-05] [G loss: 0.39378082752227783]\n",
            "[Epoch 14/30] [Batch 42/66] [D loss: 1.732340570015367e-05] [G loss: 0.4832984209060669]\n",
            "[Epoch 14/30] [Batch 43/66] [D loss: 1.739489471219713e-05] [G loss: 0.4408387541770935]\n",
            "[Epoch 14/30] [Batch 44/66] [D loss: 1.7717049558996223e-05] [G loss: 0.4024898111820221]\n",
            "[Epoch 14/30] [Batch 45/66] [D loss: 1.7424215002392884e-05] [G loss: 0.4158795177936554]\n",
            "[Epoch 14/30] [Batch 46/66] [D loss: 1.7058043340512086e-05] [G loss: 0.35766875743865967]\n",
            "[Epoch 14/30] [Batch 47/66] [D loss: 1.7860411389847286e-05] [G loss: 0.3964354991912842]\n",
            "[Epoch 14/30] [Batch 48/66] [D loss: 1.7684646081761457e-05] [G loss: 0.36418959498405457]\n",
            "[Epoch 14/30] [Batch 49/66] [D loss: 1.777548732206924e-05] [G loss: 0.37153056263923645]\n",
            "[Epoch 14/30] [Batch 50/66] [D loss: 1.7756134184310213e-05] [G loss: 0.4190206527709961]\n",
            "[Epoch 14/30] [Batch 51/66] [D loss: 1.6173233234439977e-05] [G loss: 0.41685160994529724]\n",
            "[Epoch 14/30] [Batch 52/66] [D loss: 1.740622246870771e-05] [G loss: 0.4489855170249939]\n",
            "[Epoch 14/30] [Batch 53/66] [D loss: 1.8264204300066922e-05] [G loss: 0.5191299915313721]\n",
            "[Epoch 14/30] [Batch 54/66] [D loss: 1.8071059457724914e-05] [G loss: 0.439259797334671]\n",
            "[Epoch 14/30] [Batch 55/66] [D loss: 1.754347249516286e-05] [G loss: 0.43494272232055664]\n",
            "[Epoch 14/30] [Batch 56/66] [D loss: 1.8215896488982253e-05] [G loss: 0.4217424988746643]\n",
            "[Epoch 14/30] [Batch 57/66] [D loss: 1.7187928278872278e-05] [G loss: 0.41021114587783813]\n",
            "[Epoch 14/30] [Batch 58/66] [D loss: 1.6894182408577763e-05] [G loss: 0.4291459023952484]\n",
            "[Epoch 14/30] [Batch 59/66] [D loss: 1.7976741219172254e-05] [G loss: 0.43435484170913696]\n",
            "[Epoch 14/30] [Batch 60/66] [D loss: 1.6719524865038693e-05] [G loss: 0.47536352276802063]\n",
            "[Epoch 14/30] [Batch 61/66] [D loss: 1.6771554328443017e-05] [G loss: 0.44478359818458557]\n",
            "[Epoch 14/30] [Batch 62/66] [D loss: 1.714222798909759e-05] [G loss: 0.5229657888412476]\n",
            "[Epoch 14/30] [Batch 63/66] [D loss: 1.629827147553442e-05] [G loss: 0.4272846579551697]\n",
            "[Epoch 14/30] [Batch 64/66] [D loss: 1.6755490833020303e-05] [G loss: 0.40752243995666504]\n",
            "[Epoch 15/30] [Batch 0/66] [D loss: 1.7723282326187473e-05] [G loss: 0.39161038398742676]\n",
            "[Epoch 15/30] [Batch 1/66] [D loss: 1.731559677864425e-05] [G loss: 0.46213531494140625]\n",
            "[Epoch 15/30] [Batch 2/66] [D loss: 1.7343814761261456e-05] [G loss: 0.43466177582740784]\n",
            "[Epoch 15/30] [Batch 3/66] [D loss: 1.698752475931542e-05] [G loss: 0.3607027530670166]\n",
            "[Epoch 15/30] [Batch 4/66] [D loss: 1.702240751910722e-05] [G loss: 0.4187184274196625]\n",
            "[Epoch 15/30] [Batch 5/66] [D loss: 1.7309064787696116e-05] [G loss: 0.3895380198955536]\n",
            "[Epoch 15/30] [Batch 6/66] [D loss: 1.6784706531325355e-05] [G loss: 0.40467560291290283]\n",
            "[Epoch 15/30] [Batch 7/66] [D loss: 1.7131476852227934e-05] [G loss: 0.39868178963661194]\n",
            "[Epoch 15/30] [Batch 8/66] [D loss: 1.6746204892115202e-05] [G loss: 0.3789839744567871]\n",
            "[Epoch 15/30] [Batch 9/66] [D loss: 1.7451230633014347e-05] [G loss: 0.4170045852661133]\n",
            "[Epoch 15/30] [Batch 10/66] [D loss: 1.654698371567065e-05] [G loss: 0.38872066140174866]\n",
            "[Epoch 15/30] [Batch 11/66] [D loss: 1.676062947808532e-05] [G loss: 0.3823034167289734]\n",
            "[Epoch 15/30] [Batch 12/66] [D loss: 1.6206926375161856e-05] [G loss: 0.3827955722808838]\n",
            "[Epoch 15/30] [Batch 13/66] [D loss: 1.7002512322505936e-05] [G loss: 0.4266560673713684]\n",
            "[Epoch 15/30] [Batch 14/66] [D loss: 1.7234674487554003e-05] [G loss: 0.4142338037490845]\n",
            "[Epoch 15/30] [Batch 15/66] [D loss: 1.6830533240863588e-05] [G loss: 0.42465710639953613]\n",
            "[Epoch 15/30] [Batch 16/66] [D loss: 1.58352358994307e-05] [G loss: 0.4214796721935272]\n",
            "[Epoch 15/30] [Batch 17/66] [D loss: 1.6790184417914134e-05] [G loss: 0.4370957612991333]\n",
            "[Epoch 15/30] [Batch 18/66] [D loss: 1.6332264294760535e-05] [G loss: 0.407146692276001]\n",
            "[Epoch 15/30] [Batch 19/66] [D loss: 1.6622396287857555e-05] [G loss: 0.41724562644958496]\n",
            "[Epoch 15/30] [Batch 20/66] [D loss: 1.6479104488098528e-05] [G loss: 0.36903464794158936]\n",
            "[Epoch 15/30] [Batch 21/66] [D loss: 1.6683151443430688e-05] [G loss: 0.4763181209564209]\n",
            "[Epoch 15/30] [Batch 22/66] [D loss: 1.6478998986713123e-05] [G loss: 0.4035975933074951]\n",
            "[Epoch 15/30] [Batch 23/66] [D loss: 1.591021464264486e-05] [G loss: 0.478992760181427]\n",
            "[Epoch 15/30] [Batch 24/66] [D loss: 1.631816940061981e-05] [G loss: 0.4469887316226959]\n",
            "[Epoch 15/30] [Batch 25/66] [D loss: 1.6337743545591366e-05] [G loss: 0.4259619414806366]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-24c74e15a7c9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(older_dataloader)}] \"\n\u001b[0;32m---> 80\u001b[0;31m               f\"[D loss: {loss_D_X.item() + loss_D_Y.item()}] [G loss: {loss_G.item()}]\")\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training한 모델 저장하기"
      ],
      "metadata": {
        "id": "flaH_pXvj_Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(cycle_gan.state_dict(), 'cycle_gan5.pth')"
      ],
      "metadata": {
        "id": "NBOXzD3M4oge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### trained된 모델로 이미지 넣어 결과물 보기"
      ],
      "metadata": {
        "id": "14JZOEIFkB7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 저장된 모델 불러오기"
      ],
      "metadata": {
        "id": "6xGUj9ZrkJOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Assuming you have defined your CycleGAN model class (CycleGAN) and its architecture\n",
        "\n",
        "# Instantiate the CycleGAN model\n",
        "cycle_gan = CycleGAN()\n",
        "\n",
        "# Load the trained model weights\n",
        "model_path = '/content/cycle_gan4.pth'\n",
        "\n",
        "cycle_gan.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "cycle_gan.eval()\n",
        "\n",
        "# Move the model to CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cycle_gan.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQC0PY14hvyy",
        "outputId": "da8d872b-228e-48a4-d685-ff0182e10af6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CycleGAN(\n",
              "  (G_XtoY): Generator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU(inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): ReLU(inplace=True)\n",
              "      (11): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (16): ReLU(inplace=True)\n",
              "      (17): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (19): ReLU(inplace=True)\n",
              "      (20): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (21): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (G_YtoX): Generator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU(inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): ReLU(inplace=True)\n",
              "      (11): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (16): ReLU(inplace=True)\n",
              "      (17): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (19): ReLU(inplace=True)\n",
              "      (20): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (21): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (D_X): Discriminator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "      (12): Sigmoid()\n",
              "    )\n",
              "  )\n",
              "  (D_Y): Discriminator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "      (12): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "input 이미지 사진 transform하고 tensor로 변환하기"
      ],
      "metadata": {
        "id": "UC2Ww7m9kMSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the input image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize to the model's input size\n",
        "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load and preprocess the test image\n",
        "input_image_path = '/content/drive/MyDrive/deep_learning/97734_314000_4035 (1).jpg'\n",
        "input_image = Image.open(input_image_path).convert('RGB')\n",
        "input_tensor = transform(input_image).unsqueeze(0).to(device)\n"
      ],
      "metadata": {
        "id": "H7_eAuFth62r"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### output tensor 다시 이미지로 변환하고 결과물 저장하기"
      ],
      "metadata": {
        "id": "nF3HX1FRkQpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # Perform translation\n",
        "  output_tensor = cycle_gan.G_XtoY(input_tensor)  # Assuming you want to translate to domain Y\n",
        "\n",
        "  # Convert the output tensor to a PIL image\n",
        "  output_ts = output_tensor.squeeze().cuda()\n",
        "  output_image = transforms.ToPILImage()(output_ts*0.5+0.5)\n",
        "\n",
        "  # Display or save the translated image\n",
        "  output_image.show()\n",
        "\n",
        "  # Or save it to a file\n",
        "  output_image.save('translated_image4.png')\n"
      ],
      "metadata": {
        "id": "wboNI6rnh2Oy"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}